var store = [{
        "title": "Hybrid Neural Network-Type-2 Fuzzy Logic for Maritime Weather Forecaster",
        "excerpt":"   This projects was from my bachelor thesis that is about apply hybrid method of Artificial Neural Network and Fuzzy Logic that weighted and optimized by Differential Evolution algorithm for calculating best weight for the hybrid models. This method was applied in maritime weather forecast of rain intensity, temperature, humidity, wind speed and direction.   [Paper] [Code]  ","categories": [],
        "tags": [],
        "url": "https://mrifkikurniawan.github.io/portofolio/maritime_weather_forecaster/",
        "teaser": null
      },{
        "title": "Multi-label Classifier Trainer Pytorch Lightning",
        "excerpt":"Implementation of multi-label classifier on top of Pytorch Lightning.   [Code]  ","categories": [],
        "tags": [],
        "url": "https://mrifkikurniawan.github.io/portofolio/multilabel_classifier_pytorchlighting/",
        "teaser": null
      },{
        "title": "Learning With Retrospection PyTorch Lightning",
        "excerpt":"Unofficial implementation of Learning With Retrospection research.   [Code]  ","categories": [],
        "tags": [],
        "url": "https://mrifkikurniawan.github.io/portofolio/learning_with_retrospection/",
        "teaser": null
      },{
        "title": "Online Continual Learning via Multiple Deep Metric Learning and Uncertainty-guided Episodic Memory Replay",
        "excerpt":"3rd Place Solution for International Conference on Computer Vision (ICCV) 2021 Workshop SSLAD Track 3A Continual Object Classification.   [Code] [Technical Report] [Slide] [Video]  ","categories": [],
        "tags": [],
        "url": "https://mrifkikurniawan.github.io/portofolio/continual_sslad/",
        "teaser": null
      },{
        "title": "From Cuneiform to Binary, from Stone to Cloud Memory",
        "excerpt":"[Medium]      Just imagine we lived in the 7000 years ago when no letters of alphabet A, B, C to Z were found, or we could not count the number of our fish caught because there were no known names such as numbers 1,2,3 or 9. Then how do we convey event A or information B and then, furthermore, store it so that it becomes our track record in the future?    In the beginning, humans only relied on the most important organ of themselves as storage memory, the brain. Though the brain has 3 fundamental flaws according to Yuval Noah Harari. First, the brain has limited capacities. Up to now, researchers are still unable to determine how much memory capacity our brains have. According to Paul Reber, professor of psychology at Northwestern University, which was reported by Scientific American, the human brain consists of about one billion neurons, each of which is connected with 1000 other neurons; thus, forming about 3 trillion connections. If quantified in data size, it can be estimated that around 2.5 petabits (million gigabits), which means that capacity can be used to record millions of hours of video. However, it was all just an estimate; the researchers still could not determine the actual size because we do not know how to measure actual memory. Meanwhile, certain experiences require more detail so that they require more space in the brain so that other experiences are forgotten and free up to certain spaces in the brain.   Secondly, the information stored in the brain will be destroyed when someone who owns the brain dies. If Einstein lived 7000 years ago and assumed that he had found the postulate of the workings of the universe through the theory of general and special relativity. It is almost certain that the theory will not be known by humans of modern civilization. It might be possible to find it, but it will take several hundred years to uncover it with a large investment of research. We must find traces of historical information and data without writing, drawing, painting because writing as a manifestation of ideas that were not discovered yet at that time. Einstein’s ideas will die when Einstein also dies.   And third, the human brain is adapted to store and process certain types of information. By learning, fruit traders can differentiate between rotten and fresh fruits by just seeing or feeling a few parts. Or a snake charmer who becomes a master handler by studying snake behavior, when he can be subdued and not.   The Emergence of Clay and Stone Letters and Storage Technology  The writings were first discovered by the Sumerian people who lived in the southern region of Mesopotamia around 3400–3000 BC. By scientists, Sumer is considered the first urban civilization in the world, Ancient Mesopotamia as a place where civilization emerged. This article was later called cuneiform. The geniuses there for the first time used this letter to count and record. In his book “Sapiens”, Yuval Noah Harari explained that Sumerian’s first manuscripts only contained documents about the economy, tax payment records, debt accumulation, and property ownership.                 Cuneiform, the oldest letter system in the world. Source: https://www.historyextra.com/.    The Sumerian writing system stores information with two marks printed on clay plates. However, with this breakthrough, the Sumerians were able to store far more data than any human brain. Nobody expected this article to make an important momentum of change in the civilization of all time. Until then developed and produced writing that was created in the Mesopotamian era like The Epic of Gilgamesh, Atrahasis, The Descent of Inanna, The Myth of Etana, and The Enuma Elishiyang written in cuneiform. It takes several centuries of research to be understood by modern humans.   Until the middle of the 19th century, George Smith (1840–1876) and Henry Rawlinson (1810–1895) translated The Epic of Gilgamesh into English. Which then becomes momentum the opening of Mesopotamian historical data. This is the beginning of the writing which is then followed by variations of writing and language that we know today. Writing makes a man able to manifest information in his brain so that it becomes a solution to the deficiency of the brain and reduces brain function as a place to store information.   In the Hindu-Buddhist kingdoms civilization in Indonesia, royal information such as the names of kings or stories of royal struggles is written. Imagine if this story is stored in the memories of the governors, maybe the story will not last long. And it might not be enjoyed by children and grandchildren for hundreds of years to come. However, the writing was manifested in stone which we call inscriptions and papers which when compiled into books make the age of information last longer, beyond human life.   If the internet has been discovered, maybe a royal IT expert will create a royal website and write down all the information in it. Academics owned by the kingdom will write a biography of one of the kings and write historical events and the royal officials will always record the income and expenditure of the king’s finances through word excel from his computer. All information will be recorded and read by future generations. After that, all documents will be uploaded to cloud-based storage to secure the data.   However, the human brain has not developed that far. The best technology for long-lasting data storage is still in the form of stones, tree trunks, and when it’s better to find books. In the 14th century, in the kingdom of Majapahit, we can find traces left in the form of books like Kakawin Nagarakrtagama by Empu Prapanca. The book makes it easy for the current generation to learn about life in Majapahit and uncover important events. This proves the availability of recorded information is very helpful in the world of research in revealing how people used to be active and working, the political and economic system, or the government system.   When Binary Letters Meet Silicon and Transistors  At present, we are in the era of transistors, silicon, and binary numbers. Information and data can not only be recorded in writing but also images and videos. Someone who today records himself eating at a traditional flavored Javanese restaurant using his Apple smartphone and uploading it to an Instagram account will forever be a track record that can be opened 100 or 500 years later.   As one Middle Eastern expert did, who can determine where hoax news is based on images in articles that are compared to the same images in certain articles using Google Image and tracks the biodata of a terrorist child and his affiliates from his photo and social media accounts using the same method for tracking the originality of an article.   The development of human civilization can not be separated from the human ability to simplify all forms of information into two numbers, 1 and 0. Two numbers that are enough to make information last forever. With the help of the formation of an internet network that can be regarded as a worldwide network of information people become goods that are no longer valuable because the amount is very abundant. Internet networks are revolutionizing the way information spreads, the presentation of information, and the age of information. With just a few fingers touch on the smartphone layer, one can make threats to a country.   The internet, which basically uses binary numbers in its information transmission, with pulses of the speed of light transmitted through optical fibers as it is now, makes information more quickly conveyed by the construction of complex networks. This makes information can be archived to other computers.   Today we know of Cloud technology, for example, owned by large companies such as Google, Amazon, or Alibaba. If they’re used to be a library that had to be destroyed to conquer and win the war. In the future cloud server might be the main target in winning the war.   Today we cannot only store information in written form. Even pictures and moving images (video) is the best way to store information. In the past, humans used language and writing to describe the atmosphere of events around them. There can be misinterpretations there, but since humans are able to record. Information storage of an event can be directly seen without the help of human reasoning.   So what happens is eternity and ease in getting data and storing it. One day, 1,000 or 10,000 years to come to the next generation will not be difficult to explore and recognize human behavior now. All are available, all easily accessible. The abundance of information will shift the problem for the next generation from the initial allocation of energy to search to the need for more energy to assess and classify.  ","categories": [],
        "tags": ["History"],
        "url": "https://mrifkikurniawan.github.io/blog-posts/From_Cuneiform%20to_Binary,%20from_Stone_to_Cloud_Memory/",
        "teaser": "https://miro.medium.com/max/620/1*J6q_C4-gbvXS2_krJdxU9A.jpeg"
      },{
        "title": "Industry 4.0-The Communicating Machines",
        "excerpt":"[Medium]   It All Started From a Sewing Machine  In 1589, Queen Elizabeth I refused to grant a patent on someone’s intelligent invention, he was William Lee, who was able to model hand movements when knitting on a machine. The Queen refused on the grounds that this machine would damage the conventional textile industry. After that, 200 years later this tool revolutionized the way the textile industry worked, the machine that had been rejected by the queen to be the most used machine in all textile industries at that time. By using massively this technology, the textile industry is able to mass-produce fabric in a faster time. This then became the most important momentum in initiating the first industrial revolution.   Late in the 18th century, to be exact, the first industrial revolution took place. This event has inherited steam technology and engine-based power that can replace the strength of human muscle. This first industrial revolution was based on Newton’s discovery when formulating the law of motion. Since then, the motion can be better understood and measured, which then allows humans to make steam engines that can mechanize the work previously done by humans (Xing and Marwala, 2017). This first industrial revolution can be said to be the birth of a machine.   Great Britain became the first region to face this revolutionary change which later spread to Europe and America. This change has penetrated every aspect of life, not apart from the jobs and occupations which then changed the entire social order and psychology of society.   The lower-middle-class workers who rely more on the ability of muscles replaced by the machines. The first industrial revolution resulted in at least 1/3 of the total workers were laid off no work and another 1/3 working part-time because of the ability to work have been taken over by machines. The utilization of the steam machines increases the need for coal which then raises the problem most often discussed today, global warming.                 The rate of birth and death of the population during and after the industrial revolution (Easton, 2013).    From the demographic aspect, in 1750 the number of British people 6.5 million people increased by 400% in 1990 to 32.5 million people. In 1750, 80% of rural people lived in villages and 20% of people lived in cities, turned 180 degrees in 1850, 80% of people lived in cities and 20% of people lived in villages.   The habit of the people who initially farmed and relied on food and food supplies from agricultural products turned into an industrial labor society working in factories and offices. Up to now, we can see the impact of the industrial revolution from the phenomenon of urbanization and dominated employment patterns in the industrial sector.   The Material Revolution and the Mobilization Revolution  100 years later, Sir Henry Bessemer discovered a new method of making steel from molten scrap metal. This method revolutionized steel manufacturing to be cheaper, increase the scale and speed of production, and reduce the need for workers in their manufacture (Mokyr, 1998).   As a result, the mass production of steel took place. Steel replaces iron which is proven to be weaker and more expensive. Steel which is proven to be cheaper and more durable enables the construction of railroad lines to be cheap, as a result, the acceleration of its construction can no longer be avoided. The existence of railroads and development that is increasingly equitable changes the pattern and intensity of transportation and supply chains which results in a decrease in commodity prices such as coal and steel.   This transportation system also revolutionized the mobilization of all things, including in the dissemination of information and knowledge. This discovery allows more humans to migrate for certain purposes such as learning. This then also changes how science and technology spread. Because the cheaper the means of transportation, the more easily accessible transportation, the more people and goods move at an affordable price. As a result, economic growth has increased and competency and expertise have also increased.   Faraday and Maxwell accelerated this two-volume industrial revolution through their magnetic and electrical forces which created the generation of electricity and electromagnetism that are widely used in industrial systems in the future (Xing and Marwala, 2017). This electricity supports the industry for mass production (Schwab, 2016).   Electrification is also the cause of the creation of major changes in industrial production methods, which are called the assembly line and mass production. Elective also gives the industry the ability to be able to work for 24 hours without stopping. This means that work time is also increasing for employees. They can work up to 10–16 hours a day. This brings workers to bad conditions such as low wages, poor working conditions (poor lighting, and no machine safety). This results in widespread poverty and very high depression.   Communication Revolution  Rapid mobilization has accelerated the spread of science and technology. The next era is the era of transistors. The transistor initiated the invention of computers and the internet. Transistors have led humans to the invention of computers that have become the fastest data processing machines ever. In the same era, the internet was discovered, the most massive distributor of information could be realized. The internet is changing the way humans spread information and knowledge.      Energy has become the basis of the second industrial revolution, while the third industrial revolution is the era of information and knowledge    Letters used to be very useful in sending news or spreading information. It takes days and even months to wait for a letter to be delivered to a specific destination. At that time the inter-mail system was still a unity with the transportation system, so the letter was very dependent on the development of the transportation system. Likewise knowledge, someone needs to go overseas and travel hundreds of kilometers to meet a teacher and get knowledge from him. After that, just back to the area to disseminate the knowledge gained. However, since the discovery of the internet, everything has changed. It only takes a few minutes for information to be conveyed. Content and science books available on computer servers on the internet can be downloaded and studied with just one or two clicks. The internet makes the earth turn faster.   Acceleration of knowledge has implications for the acceleration of innovation. This era makes the machine capable of sensing and making decisions individually. Computer technology takes the machine to the next generation, namely machine automation. Computers carrying machines in the industry can work without human control in accordance with what has been programmed by humans. At that time, data and information transfers were only limited to one factory system.   Intersystem data transfer is still not possible due to limitations in data transmission technology. Next is the era in which these systems communicate with each other and each individual system is able to learn and develop like humans. What was this era like?   Industrial Revolution 4.0  Today, we are at the end of industry 3.0 and are on a journey of transition to a change in industry style that we could not have foreseen, the cyber-physical industry 4.0. The third industrial revolution has been able to make the glass doors of offices open automatically or to be able to close the reservoir water in our homes when fully charged. Then what time will we face after this?   Imagine if we are leaving for work, then you call the car you have through a smartphone to take you and stop right in front of you. On the trip, you enjoy a cup of coffee that has been provided by the coffee maker every morning in your car and access the latest information through the screen projected on the windshield of your car. Meanwhile, your car is moving at a steady speed with self-driving capability without stopping due to traffic lights.   Yes, each car is able to communicate with each other so that it allows them to go without colliding. Such is the picture of industry 4.0 when all things have been made automatically, the next step is to make them able to communicate with each other like humans reprimand their friends when they meet or stay away from those who have problems with their health.   In 2011 at an event titled “Hannover Mesese” the German government launched a completely new concept of the future of the industry. Digitalization of manufacturing was adopted as the theme which became the basis of this future industrial concept, they named it “Industrie 4.0”. The academics and business leaders who poured the concept did not know that the concept would spread quickly and become a future industrial concept of the world. 5 years later, the concept became viral and was widely discussed at conferences and seminars related to the concept of the future industry.   Industry 4.0 is a new paradigm in the revolutionary way of industry. This paradigm creates the realization of integration and interconnection between machines, products, components, individuals, and information technology that are united in industrial systems. Integration, interconnection, and flexibility are the principles that this series of four industrial revolutions will bring. The fourth industrial revolution can be said to be machines that can communicate with each other.   This interaction allows traffic lights located in East Surabaya to communicate with traffic lights in West Surabaya to resolve traffic jams during work hours. Industry 4.0 can be said to be the next digitization of the manufacturing sector, which is controlled by 4 things: data empowerment, computing power, and connections; analytical and business intelligence capabilities; new ways of human interaction with machines such as touch layers and Augmented Reality; and increased transfer of data instructions to the physical world such as robotics and 3D printers (Lee et al., 2013).                 Industry Principles 4.0 (Cohen et al., 2017).    In industry 4.0, data will be the most valuable. Data in the form of numbers and letters that can be interpreted into information. This interpretation will be very useful in making decisions resulting from the processing of AI or Machine Learning. E-Commerce currently for example has used data to predict demand and determine the price of goods in accordance with the number of requests and purchases that are continuously monitored from the virtual activities of its customers. This is evidence of the realization of flexibility in industry 4.0 and business principles that are customer-oriented by understanding the behavior and activities of customers.   In the industrial era 4.0, technology development will refer to the fields of genetics, biotechnology, Artificial Intelligence, robotics, nanotechnology, and 3D printing (World Economic Forum, 2016). These technologies enable humans to work more efficiently and cheaper than they ever have. Artificial intelligence, for example, allows a computer to make decisions like an expert or expert system.   If humans can realize this, then authenticity will quickly be distributed to all computers and machines. The ability of experts to predict the surge in demand for goods or the value of the most appropriate goods is easily applied and disseminated. We only need a hardware device that has sensors and is connected to the internet (IoT). Then the AI software that is installed on hardware downloaded through the internet will manage that data to be the most effective and efficient expert decision ever made.                 Application of industrial sectors 4.0. (World Economic Forum and International BVL, 2017).    From Centralization Towards Decentralization  These years maybe the company’s production operations are still centralized. In this case, they need a fast distribution system to send their products. However, the next few years 3D printing will change it. 3D printing allows an automotive factory to produce goods in places that are affordable to customers. This will be able to minimize the distribution budget from the previous era.   Prediction using AI will enable companies to determine the best production points, complete with the number of items needed by the customer at that time. Here, 3D printers that have been placed at certain points will work according to requests sent from AI processed data. Each printer works according to the design that the company has sent to its software through a fiber optic communication network that connects the points of the production plant.   Changes in the pattern of production that are centralized in one point of production into a number of points of production which is a forecast of future production patterns. Each machine installed with AI software or machine learning will be able to make decisions individually based on centralized collective data (integration).   So that intelligent machines this time will be able to replace not only menial work but also the ability of logic and human thinking. The World Economic Forum with BVL International predicts the impact of the fourth industrial revolution on certain sectors such as employment, manufacturing and industry, supply chain, services and business models, and the company’s relationship with customers. Industry 4.0 will change the most essential things that humans have (Schwab, 2016).      Then what about humans? Are they ready to beat the expertise of machines and robots or even get rid of the robots they make?    Reference  Cohen, Y. et al. (2017) ‘Assembly system configuration through Industry 4.0 principles: the expected change in the actual paradigms’, IFAC-PapersOnLine. Elsevier B.V., 50(1), pp. 14958–14963. doi: 10.1016/j.ifacol.2017.08.2550.   Easton, M. (2013) The Industrial Revolution — Oxford Big Ideas Geography. Oxford.   Mokyr, J. (1998) ‘The Second Industrial Revolution, 1870–1914’, (August 1998), pp. 1–19.   Schwab, K. (2016) ‘The Fourth Industrial Revolution’, World Economic Forum, p. 199. doi: 10.1017/CBO9781107415324.004.   World Economic Forum (2016) ‘The Future of Jobs Employment, Skills and Workforce Strategy for the Fourth Industrial Revolution’, Growth Strategies, (january), pp. 2–3. doi: 10.1177/1946756712473437.   World Economic Forum and BVL Interntional (2017) ‘Impact of the Fourth Industrial Revolution on Supply Chains’, (October), p. 22.   Xing, B. and Marwala, T. (2017) ‘Implications of the Fourth Industrial Age on Higher Education’, ResearchGate, (April), pp. 2–9. Available at: https://www.researchgate.net/publication/315682580%0D.   http://webs.bcp.org/sites/vcleary/modernworldhistorytextbook/industrialrevolution/ireffects.html   http://ushistoryscene.com/article/second-industrial-revolution/   http://www.historyofinformation.com/expanded.php?id=3634  ","categories": [],
        "tags": ["Technology","4th Industrial Revolution","Industrial Revolution"],
        "url": "https://mrifkikurniawan.github.io/blog-posts/Industry_4.0-The_Communicating_Machines/",
        "teaser": "https://miro.medium.com/max/700/1*WCaxYRsolx7uTWmvaUwNSg.jpeg"
      },{
        "title": "Catastrophic Forgetting in Neural Networks Explained",
        "excerpt":"   The existing neural networks is trained on top of a useful assumption of i.i.d setting while contrasting with sequential continual learning problem setting. As a result, the neural networks trained on continual tasks setting will suffer from catastrophic interference which means the networks forget how to do previously learned tasks when they encounter new tasks. This article will dig deep into the reason for forgetting, how to measure this problem, and introduced some available approaches proposed to reducing the abandoning of prior knowledge.       [Updates]  06-05-2021: Initial article publication     What is Catastrophic Forgetting?   How humans learn is both extremely fascinating and mysterious especially when it comes to the capability to continuously learn new knowledge and skills without forgetting the past experiences. As an example, while we observe the physics phenomena such as the gravitation mechanism and, afterward, acquire new knowledge how the chemistry works, we are able to remember what gravitation is about and explain it effortlessly. In contrast, from the learning intelligence machine perspective, deep learning scientists highly struggle to incorporate the lifelong learning ability into machine learning architecture such as neural networks.   The catastrophic forgetting or alternatively called catastrophic interference was observed initially by McColskey and Cohen [1] in 1898 on shallow 3-layers neural networks who realized that connectionist networks — a common term in 19’s substituting ‘neural networks’ — trained on sequential learning prone to erase the past learned knowledge. They concluded that adjusting networks weights representing the old knowledge while training caused catastrophic interference and it was precipitated and compounded by distributed representation as the recognized useful properties of Multi-layer Perceptrons.   Later, this is considered as a more expanded discipline of ‘plasticity-stability dilemma’ [2]. As a means of the study of tuning the parameters by discovering the most optimum learning algorithm to let the neural networks acquire new knowledge and be sensitive to distributional shifting — known as plasticity — but maintaining the past knowledge to reduce the forgetting — known as stability. Highly plastic networks potentially suffer from forgetting the past encoded knowledge and oppositely very stable networks could be trouble with efficient information encoding at synapse level [3].   In contrast, cognitive sciences see beyond the field as studying determining whether the earlier acquired knowledge in life is more memorized than the knowledge acquired in the coming age or called ‘The Entrenchment Effect’ [3]. Therefore, it seems a little bit different between what plasticity-stability stands for in deep learning and the cognitive science community.   While the neural networks adapt flexibly to the new incoming knowledge, it will serendipitously experience catastrophic forgetting. Conversely, networks that are prone to being unable to discriminate the new incoming inputs if the networks are extremely stable or commonly known as catastrophic remembering [4].   Contemporarily, deep learning is trained on top of a weak but useful assumption of i.i.d (independent and identically distributed) setting which means that the data points are supposed to be mutually independent  — single data is unrelated to other data point — and having similar distribution e.g. training data is assumed to have equivalent distribution to test data. Therefore, the common training setting takes the batch of samples and updates the model parameters with respect to the loss value on this batch. However, the assumption is not applicable for real-time application such as sequentially data stream training settings just like continual learning and accidentally leads to catastrophic forgetting.   Shortly, catastrophic forgetting is the radical performance drops of the model $f(X;\\theta)$ which parameterized by $\\theta$ with input $X$ — mostly neural networks exhibit distributed representation [1] — that map $X \\rightarrow Y$ performing on previously learned tasks $t_{t}$ after learning on task $t_{n}$ where t &lt; n.                     Figure 1. Continual learning task setting is designed for the model to learning multiple tasks incrementally which each individual task encompass a set of some classes            Consider as an illustration on figure 1 above, our neural networks train to discriminate between two classes of cat and dog. Therefore, the network is trained on bunches of datasets containing any variants of cat and dog for some epochs. Thereafter we want our model to recognize 2 additional classes of tiger and elephant in task 2. Hence, we should train the model with task 2 dataset holding batches of samples of tiger and elephant. In the continual learning setting, we are not allowed to train the model on both task datasets and getting access to the existing dataset only — cluster of tigers and elephants images in this case. As a result, the model will update the parameters to optimizely perform good at present task or task 2 and forget how to predict the task 1 classes given task 1 dataset; therefore, reducing the performance on task 1 or called catastrophic forgetting.   How Do Neural Networks Forget?   Mostly the standard approach for training the neural networks model is using standard backpropagation with gradient-based optimization in particular stochastic gradient descent (SGD) [5] or more sophisticated one like Adam [6]. Updating parameters via SGD as below   \\[\\theta \\leftarrow \\theta - \\eta\\frac{\\partial\\mathcal{L}}{\\partial\\theta},\\]  require $\\eta$ for tuning the updating magnitude or called learning rate on the parameters gradient $\\frac{\\partial\\mathcal{L}}{\\partial\\theta}$. However, these networks trained by gradient-based optimization algorithms are prone to encounter catastrophic forgetting. The common reason is coming from the primary factor of parameters drift while the neural networks train by taking steps to updating parameters aiming to minimize the loss on task $t$. Thanks to Masana et al [7] briefly summarize the factors of forgetting, those are including parameters shifting, logits shifting, and Inter-domain/inter-task confusion.   Parameters shifting   While the networks are being trained on the current task, the parameters will be tuned with respect to loss value in the current training dataset task. It means that the networks are optimized to perform maximum on the current task by changing the parameters. As a result, the optimization and parameters update will not consider previous task distribution which lead to forgetting how to do preceding tasks.   Logits shifting   The direct ramification of parameters shifting outputs distribution deviation of the logits given the certain input e.g., image of the previous task. In the effort to alleviate this detriment, distilling the knowledge [8] of the previous model parameters respecting the old inputs squeezes the logits outputs of the current model to be equal to the previous model logits while allowing the parameters inconsistent to the old model.   Inter-domain/inter-task confusion                    Figure 2. The networks are susceptible to misclassify task 3 (right) classes due to the model having not trained to create discriminative decision boundary for 4 classes in task 3 because of sequential learning on task 1 and task 2 separately. Source: Masana, M. et al.            The decision boundary adjustment leading to inter-task or inter-domain misclassification due to sequential learning setting on continual learning.                     Figure 3. Catastrophic forgetting in binary classification while the networks are trained on task 2 suffering from distributional shift which leads to forgetting to do discrimination on task 1. Source: Kolouri, S. et al            Take an example of a binary classification task — predicting whether given input X resulting discrete label 0 or 1 — as illustrated in the figure 3 (missing reference) above, at the beginning the networks learn to predict the dataset distribution on task 1 in such a way that resulting the model $f(X;\\theta_{0})$ with obtained parameters $\\theta_{0}$.  Then whenever the model acquires the new knowledge from dataset distribution on task 2 without certain continual learning technique, it will suffer from catastrophic forgetting on distribution dataset on task 1 due to parameters drift as consequence of distribution drift which lead to accidentally changing the decision boundary. In contrast, the ideal case should be like the right image in figure 3 which the model performs well by generating a decision boundary that captures discriminative features on both distributions. This setting can be conveniently achieved on multi-task learning settings while running the training on both dataset distributions but highly difficult for continual learning.   Measuring Catastrophic Forgetting   How to measure catastrophic forgetting could perhaps be separated into two perspectives thus quantifying to what extent the networks model is able to acquire new knowledge without forgetting and the other examine how fast the networks models adapt to past knowledge while relearning the past task after training on present task, both measurements called retention and relearning respectively [9].   Retention   Retention is most commonly used as a measuring technique for continual learning including incremental class learning or task incremental learning in the machine learning community nowadays. Simply training the networks until mastering on task 1, then moving forward to task 2 and let the networks mastering on task 2 and followed by measuring the accuracy metrics on task 1 and 2 independently is categorized as one of the retention measurements [9]. Additionally, [10] proposed widely adopted measuring technique called average incremental accuracy as formalized by following equation   \\[accuracy = \\frac{1}{T}\\sum_{t = 1}^{T}A_{t},\\]  where T is the number of tasks has been encountered so far and $A_{t}$ means accuracy on tast $t$.   However, more complicated one has been proposed by [11] which introducing   \\[\\Omega_{\\text{base}} = \\frac{1}{T - 1}\\sum_{i = 2}^{T}\\frac{\\alpha_{base,i}}{\\alpha_{\\text{ideal}}},\\]  \\[\\Omega_{\\text{new}} = \\frac{1}{T - 1}\\sum_{i = 2}^{T}\\alpha_{new,i},\\]  \\[\\Omega_{\\text{all}} = \\frac{1}{T - 1}\\sum_{i = 2}^{T}\\frac{\\alpha_{all,i}}{\\alpha_{\\text{ideal}}},\\]  $T$ is the total tasks/sessions have been trained so far, $\\alpha_{new,i}$ denotes accuracy on test set for session i direcly after learning,$\\ \\alpha_{base,i}$ is the measurement of accuracy on base class/first session after learning on sesion i, while $\\alpha_{all,i}$ is accuracy metric on all session given model trained on session i, and $\\alpha_{\\text{ideal}}$ indicates the offline model accuracy on the base set, which assumes the ideal performance or sometimes many experiments in continual learning anchor multi-task learning setting as the upper-bound. While, the function of alpha ideal as divisor here for normalization for ease to compare between datasets.   $\\Omega_{\\text{base}}$ indicates the model’s retention relative to the first session given trained model in later sessions. $\\Omega_{\\text{new}}$ measures the accuracy on training session i while the model is trained on session i as well, it is used for a model’s ability to immediately recall new tasks. While, $\\Omega_{\\text{all}}$ denotes the measurement for how well the model retain all session after trained on session i.   Relearning   Frequently overlooked by existing recent experiments, relearning is another essential measure in catastrophic forgetting which was initially proposed in physiological study by Hermann Ebbinghaus known as ‘savings’ but implemented as metrics in catastrophic forgetting by Hetherington [12]. ‘Saving’ metrics measure the saved knowledge and how fast the networks relearn the past knowledge. This metric is built on top of the assumption that possibly networks are not totally unlearned the past knowledge but that their connections may save encoded important information of the past.   Practically it is measured via training the network on task 1 and task 2 sequentially, then retrain the networks on task 1 dataset and compare the time required for the network to learn task 1 on the first time against second time. Reducing time required to relearn the task 1 indicates that the networks still saved the past information.   Activation Overlap   Activation overlap initially proposed by French [13] who argue that due to distributed representation causing connectionist networks, forgetting can be measured by quantifying the overlapping in activation output. Recently, this formalized and modified by [9] by suggesting dot product of two different samples from whether intra-class or inter-class given same hidden parameters as following,   \\[s\\left( a,b \\right) = \\frac{1}{n}\\sum_{i = 0}^{n}{g_{\\text{hi}}\\left( a \\right)\\text{.}g_{\\text{hi}}\\left( b \\right)}\\]  where $g_{\\text{hi}}$ indicates hidden layer i parameters of the networks and $g_{\\text{hi}}\\left( x \\right)$ indicating activation output of input $x$ given parameters $g_{\\text{hi}}$.   Pairwise Interference   Initially proposed by [14] and then implemented by [15] given sample a and sample b pairwise interference measure how large the interference of sample b  for trained model on sample a which can be defined as follow   \\[\\text{PI}\\left( \\theta_{t};a,b \\right) = J\\left( \\theta_{t + 1};a \\right) - \\ J\\left( \\theta_{t};a \\right).\\]  Where, $\\theta_{t + 1}$ is a model obtained after training on sample b, and $J(.)$ indicates objective function.   Overcoming Forgetting in Neural Networks   Contemporarily mitigating catastrophic forgetting highly involved in subfield of machine learning so-called continual learning. Recent advancement approaches in dealing with the issue encompassing exemplar/prototypical/experience rehearsal/replay buffer, parameters regularization, and architectural modification or otherwise named modular approach. In spite of those, in the recent past one year some scientists extend the study of moderating catastrophic forgetting a.k.a. continual learning to the search of connectivity with multi-task learning [16], loss landscape approximation [16], [17], relatedness with transfer learning [18], more challenging task settings [19], [20], [21], [22], [23], [24], [25] and even expanding beyond image classification task [26], [27], [28], [29].   Rehearsal/Replay   Rehearsal/replay approach is dealing with catastrophic forgetting modestly by replaying the bunch of knowledge memory of past knowledge so-called “episodic memory”, e.g., samples of images, into the existing training steps while learning the novel knowledge e.g., new classes. Therefore, the catastrophic interference can be diminished as consequence of the updating parameters in respect of considering batch of combining existing datasets with small buffers of replayed episodic memory. Among others this technique was mostly explored and proposed in past five years in continual learning seeing its simplicity and effectiveness as baseline for continual learning experiments.                     Figure 4. Knowledge replays in the brain while sleeping involving the Neocortex and Hippocampus. Source: Parisi, G. I. et al.            The similar mechanism also occurs in our brain when sleeping since our brain will reactivate and rehearse the past freshly acquired knowledges memorized periodically in hippocampus into peripheral permanent memory in the neocortex. As suggested in theory of Complementary Learning System (CLS) [30] shown in Figure 4 above, the Hippocampus encodes recent events or experiences via fast learning and these will be unconsciously reactivated while sleeping for gradual consolidation mechanism into neocortical memory systems.   However, the most challenging in rehearsal approach is both how to sampling the most significant examples and what kind of representations from the dataset that necessarily be rehearsed into future learning phase while minimizing catastrophic interference. Many of the latest research concerned with this issue along with proposing novel sampling techniques or including random sampling, uniform sampling, reservoir sampling [31], [32], distance-based sampling [33], maximally interfered sampling [34], among others. On the other hand, replaying expressive representations involve naïve image replay, embedding replay, anchor replay, and topological/relational replay.   Regularization   Measuring the any past information, including parameters, importance relevant to both past task loss value and accuracy metrics and restricting the extreme updates to this information while learning is the other strategy named regularization approach. This is conceivably conjectured as the mechanism to control plasticity-stability dilemma of the neural networks on the subject of continual updates. As consequent, the restraint adopted to the information of interest, such as parameters, guarantee the minimization of interfered information essential for the prior task.   Up till now, according to [35], some of experiments can be clustered into prior-focus/parameters-based and data-focused/logits-based regularization. Parameters-based control the model parameters distribution and plasticity-stability. While, data-focused distil the logits (model outputs before activation function) of given inputs inferenced on the present model as manoeuvre to recall past knowledge.                     Figure 5. Training with EWC as shown on red trajectories will finding out low loss elevation in space on both task A (old) and task B (new) such that the obtained parameters are capable to perform accurately both on task A and B. Source: Kirkpatrick et al.            The earliest method proposed this idea was Elastic Weight Consolidation (EWC) [36]. The basic idea is to measure weight importance for the previous task while controlling these previous weights $\\theta_{A}^{*}$ and avoid significant updates to these weights via fisher information matrix $F$ as measure of the importance. While EWC minimize the loss of   \\[\\mathcal{L}\\left ( \\theta  \\right ) = \\mathcal{L}_{B}\\left ( \\theta \\right ) + \\sum_{i}^{} \\frac{\\lambda}{2}F_{i}\\left ( \\theta_{i} - \\theta_{A,i}^{*} \\right )^{2},\\]  where $\\mathcal{L}_{B}$ is the task B loss, $\\lambda$ denotes the relation of old task to new, $i$ is each parameter index, and $\\theta$ is the current parameters. As exhibited on the loss equation above, the new parameters will be enforced to close to old parameters to alleviate forgetting which the precision will be controlled by fisher information matrix $F$.   Architectural   While architectural-based approach mainly concerned with constructing progressive neural networks while learning novel tasks or knowledges either by growing task-specific architecture [37], producing single-independent head on classifier per class/task (missing reference), or rewiring the connections in neural networks layers while incrementally learning novel tasks [38].                     Figure 6. Progressive networks exhibit three columns networks which each column is associated to task-specific networks e.g., networks on column 1 and 2 for performing task 1 and 2, respectively. Whilst column 3 networks solve task 3 that this networks is enabled getting access to previous leaned features. Source: Rusu, A. A. et al.            Among others is Progressive Neural Networks proposed in 2016 as depicted in the figure 6 above. The progressive networks framework proposed addressing catastrophic forgetting through evolving task-specific networks instanting on a column for working on a task being solved. Then, as the task encountered is incremental growth, the novel column networks will be introduced which the previously learned features feasibly transferred to the new networks via lateral connections. Therefore, the last task with its associated networks are allowed to exploit all the features learned so far.      [Notes]  If you have any disapproval, correction, and critique to this article feel free to email me, I will happily adjusting and modifying this published contents respecting the corrections.    References   [1]M. McCloskey and N. J. Cohen, “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem,” Psychology of Learning and Motivation - Advances in Research and Theory, vol. 24, no. C, pp. 109–165, 1989, doi: 10.1016/S0079-7421(08)60536-8.  [2]R. French, “Catastrophic forgetting in connectionist networks,” Trends in Cognitive Sciences, vol. 3, no. 4, pp. 128–135, Apr. 1999, doi: 10.1016/S1364-6613(99)01294-2. [Online]. Available at: https://linkinghub.elsevier.com/retrieve/pii/S1364661399012942 [3]M. Mermillod, A. Bugaiska, and P. Bonin, “The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects,” Frontiers in Psychology, vol. 4, no. August, pp. 1–3, 2013, doi: 10.3389/fpsyg.2013.00504.  [4]P. Kaushik, A. Gain, A. Kortylewski, and A. Yuille, “Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping,” 2021 [Online]. Available at: http://arxiv.org/abs/2102.11343 [5]H. Robbins and S. Monro, “A Stochastic Approximation Method,” The Annals of Mathematical Statistics, vol. 22, no. 3, pp. 400–407, 1951, doi: 10.1214/aoms/1177729586.  [6]D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,” 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, pp. 1–15, 2015.  [7]M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov, and J. V. D. Weijer, “Class-incremental learning : survey and performance evaluation,” arXiv Preprint, pp. 1–24, 2020.  [8]G. Hinton, O. Vinyals, and J. Dean, “Distilling the Knowledge in a Neural Network,” pp. 1–9, 2015 [Online]. Available at: http://arxiv.org/abs/1503.02531 [9]D. R. Ashley, S. Ghiassian, and R. S. Sutton, “Does Standard Backpropagation Forget Less Catastrophically Than Adam?,” 2021 [Online]. Available at: http://arxiv.org/abs/2102.07686 [10]S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “iCaRL: Incremental Classifier and Representation Learning,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, vol. 2017-Janua, pp. 5533–5542, doi: 10.1109/CVPR.2017.587 [Online]. Available at: http://arxiv.org/abs/1611.07725 http://ieeexplore.ieee.org/document/8100070/ [11]R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan, “Measuring catastrophic forgetting in neural networks,” 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pp. 3390–3398, 2018.  [12]P. A. Hetherington and M. S. Seidenberg, “Is there ‘catastrophic interference’ in connectionist networks,” in Proceedings of the 11th annual conference of the cognitive science society, 1989, vol. 26, p. 33 [Online]. Available at: http://scholar.google.com/scholar?q=related:6OvJaVLsTzwJ:scholar.google.com/&amp;hl=en&amp;as_sdt=0,10#3 [13]R. French and M. French, “Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionlst Networks,” Proceedings of the AAAI Spring Symposium, no. JANUARY 1992, pp. 70–77, 1993.  [14]V. Liu, “Sparse Representation Neural Networks for Online Reinforcement Learning,” 2019.  [15]S. Ghiassian, B. Rafiee, Y. L. Lo, and A. White, “Improving performance in reinforcement learning by breaking generalization in neural networks,” Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS, vol. 2020-May, no. 1, pp. 438–446, 2020.  [16]S. I. Mirzadeh, M. Farajtabar, D. Gorur, R. Pascanu, and H. Ghasemzadeh, “Linear mode connectivity in multitask and continual learning,” in Iclr 2021, 2020 [Online]. Available at: https://arxiv.org/abs/2010.04495 https://github.com/imirzadeh/MC-SGD [17]D. Yin, M. Farajtabar, A. Li, N. Levine, and A. Mott, “Optimization and Generalization of Regularization-Based Continual Learning: a Loss Approximation Viewpoint,” pp. 1–17, Jun. 2020 [Online]. Available at: http://arxiv.org/abs/2006.10974 [18]Z. Ke, B. Liu, and X. Huang, “Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks,” NeurIPS, no. NeurIPS, pp. 1–12, 2020 [Online]. Available at: https://github.com/ZixuanKe/CAT [19]Songlin Dong, X. Hong, X. Tao, X. Chang, X. Wei, and Y. Gong, “Few-Shot Class-Incremental Learning via Relation Knowledge Distillation,” in 35th AAAI Conference on Artificial Intelligence, AAAI 2021, 2020.  [20]H. Zhao, Y. Fu, X. Li, S. Li, B. Omar, and X. Li, “Few-shot class-incremental learning via feature space composition,” arXiv, 2020.  [21]A. Bertugli, S. Vincenzi, S. Calderara, and A. Passerini, “Few-shot unsupervised continual learning through meta-examples,” arXiv, no. NeurIPS, 2020.  [22]M. Caccia et al., “Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning,” 2020 [Online]. Available at: http://arxiv.org/abs/2003.05856 https://github.com/ElementAI/osaka [23]M. Ren, R. Liao, E. Fetaya, and R. S. Zemel, “Incremental few-shot learning with attention attractor networks,” Advances in Neural Information Processing Systems, vol. 32, no. NeurIPS, pp. 1–15, 2019 [Online]. Available at: https://github.com/renmengye/inc-few-shot-attractor-public [24]A. R. Dhamija, T. Ahmad, J. Schwan, M. Jafarzadeh, C. Li, and T. E. Boult, “Self-Supervised Features Improve Open-World Learning,” 2021 [Online]. Available at: http://arxiv.org/abs/2102.07848 [25]D. Rao, F. Visin, A. A. Rusu, Y. W. Teh, R. Pascanu, and R. Hadsell, “Continual unsupervised representation learning,” Advances in Neural Information Processing Systems, vol. 32, 2019.  [26]K. J. Joseph, J. Rajasegaran, S. Khan, F. S. Khan, V. N. Balasubramanian, and L. Shao, “Incremental object detection via meta-learning,” arXiv, vol. 14, no. 8, pp. 1–8, 2020.  [27]J. M. Perez-Rua, X. Zhu, T. M. Hospedales, and T. Xiang, “Incremental Few-Shot Object Detection,” Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, no. 1, pp. 13843–13852, 2020, doi: 10.1109/CVPR42600.2020.01386.  [28]E. Zheng, Q. Yu, R. Li, P. Shi, and A. Haake, “A Continual Learning Framework for Uncertainty-Aware Interactive Image Segmentation,” in 35th AAAI Conference on Artificial Intelligence, AAAI 2021, 2021.  [29]W. Chen, Y. Liu, W. Wang, T. Tuytelaars, E. M. Bakker, and M. Lew, “On the exploration of incremental learning for fine-grained image retrieval,” arXiv, 2020.  [30]J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly, “Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights From the Successes and Failures of Connectionist Models of Learning and Memory,” Psychological Review, vol. 102, no. 3, pp. 419–457, 1995.  [31]J. S. Vitter, “Random sampling with a reservoir,” ACM Transactions on Mathematical Software, vol. 11, no. 1, pp. 37–57, Mar. 1985, doi: 10.1145/3147.3165. [Online]. Available at: https://dl.acm.org/doi/10.1145/3147.3165 [32]C. D. Kim, J. Jeong, and G. Kim, “Imbalanced Continual Learning with Partitioning Reservoir Sampling,” in ECCV, 2020, vol. 12358 LNCS, pp. 411–428, doi: 10.1007/978-3-030-58601-0_25.  [33]J. Pomponi, S. Scardapane, V. Lomonaco, and A. Uncini, “Efficient continual learning in neural networks with embedding regularization,” Neurocomputing, vol. 397, pp. 139–148, 2020, doi: 10.1016/j.neucom.2020.01.093. [Online]. Available at: https://doi.org/10.1016/j.neucom.2020.01.093 [34]R. Aljundi et al., “Online Continual Learning with Maximally Interfered Retrieval,” Advances in Neural Information Processing Systems, vol. 32, no. NeurIPS 2019, Aug. 2019 [Online]. Available at: http://arxiv.org/abs/1908.04742 [35]M. Delange et al., “A continual learning survey: Defying forgetting in classification tasks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2021, doi: 10.1109/TPAMI.2021.3057446. [Online]. Available at: http://arxiv.org/abs/1909.08383 https://github.com/Mattdl/CLsurvey https://ieeexplore.ieee.org/document/9349197/ [36]J. Kirkpatrick et al., “Overcoming catastrophic forgetting in neural networks,” Proceedings of the National Academy of Sciences of the United States of America, vol. 114, no. 13, pp. 3521–3526, 2017, doi: 10.1073/pnas.1611835114. [Online]. Available at: https://github.com/ariseff/overcoming-catastrophic https://github.com/stokesj/EWC [37]A. A. Rusu et al., “Progressive Neural Networks,” arXiv, Jun. 2016 [Online]. Available at: http://arxiv.org/abs/1606.04671 [38]M. Wortsman et al., “Supermasks in Superposition,” arXiv, no. NeurIPS, 2020.  ","categories": [],
        "tags": ["continual learning","deep learning","machine learning","neural network","catastrophic forgetting"],
        "url": "https://mrifkikurniawan.github.io/blog-posts/Catastrophic_Forgetting/",
        "teaser": "https://mrifkikurniawan.github.io/images/catastrophic_forgetting/forgetting_cl_task.jpg"
      },]
