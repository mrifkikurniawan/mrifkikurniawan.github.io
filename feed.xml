<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://mrifkikurniawan.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mrifkikurniawan.github.io/" rel="alternate" type="text/html" /><updated>2021-11-06T00:15:28-07:00</updated><id>https://mrifkikurniawan.github.io/feed.xml</id><title type="html">Profil Page</title><subtitle>AI Scientist at Nodeflux | B.Eng of Engineering Physics</subtitle><author><name>Muhammad Rifki Kurniawan</name><email>mrifkikurniawan17@gmail.com</email></author><entry><title type="html">Catastrophic Forgetting in Neural Networks Explained</title><link href="https://mrifkikurniawan.github.io/blog-posts/Catastrophic_Forgetting/" rel="alternate" type="text/html" title="Catastrophic Forgetting in Neural Networks Explained" /><published>2021-05-06T00:00:00-07:00</published><updated>2021-05-06T06:33:28-07:00</updated><id>https://mrifkikurniawan.github.io/blog-posts/Catastrophic_Forgetting_in_Neural_Networks_Explained</id><content type="html" xml:base="https://mrifkikurniawan.github.io/blog-posts/Catastrophic_Forgetting/">&lt;blockquote style=&quot;text-align: justify;&quot;&gt;
  &lt;p&gt;The existing neural networks is trained on top of a useful assumption of i.i.d setting while contrasting with sequential continual learning problem setting. As a result, the neural networks trained on continual tasks setting will suffer from catastrophic interference which means the networks forget how to do previously learned tasks when they encounter new tasks. This article will dig deep into the reason for forgetting, how to measure this problem, and introduced some available approaches proposed to reducing the abandoning of prior knowledge.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;[Updates]&lt;br /&gt;
&lt;span style=&quot;color:#8B0000&quot;&gt;06-05-2021&lt;/span&gt;: &lt;span style=&quot;color:#1E90FF&quot;&gt;Initial article publication&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;what-is-catastrophic-forgetting&quot;&gt;What is Catastrophic Forgetting?&lt;/h1&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;How humans learn is both extremely fascinating and mysterious especially when it comes to the capability to continuously learn new knowledge and skills without forgetting the past experiences. As an example, while we observe the physics phenomena such as the gravitation mechanism and, afterward, acquire new knowledge how the chemistry works, we are able to remember what gravitation is about and explain it effortlessly. In contrast, from the learning intelligence machine perspective, deep learning scientists highly struggle to incorporate the lifelong learning ability into machine learning architecture such as neural networks.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The catastrophic forgetting or alternatively called catastrophic
interference was observed initially by McColskey and Cohen &lt;a class=&quot;citation&quot; href=&quot;#McCloskey1989&quot;&gt;[1]&lt;/a&gt; in 1898 on shallow 3-layers neural networks who realized that connectionist networks — a common term in 19’s substituting ‘neural networks’ — trained on sequential learning prone
to erase the past learned knowledge. They concluded that adjusting
networks weights representing the old knowledge while training caused
catastrophic interference and it was precipitated and compounded by
distributed representation as the recognized useful properties of
Multi-layer Perceptrons.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Later, this is considered as a more expanded discipline of
‘plasticity-stability dilemma’ &lt;a class=&quot;citation&quot; href=&quot;#French1999&quot;&gt;[2]&lt;/a&gt;. As a means of the study of tuning the parameters by discovering the most optimum learning algorithm to let the neural networks acquire new knowledge and be sensitive to distributional shifting — known as plasticity — but maintaining the past knowledge to reduce the forgetting — known as stability. Highly plastic networks potentially suffer from forgetting the past encoded knowledge and oppositely very stable networks could be trouble with efficient information encoding at synapse level &lt;a class=&quot;citation&quot; href=&quot;#Mermillod2013&quot;&gt;[3]&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;In contrast, cognitive sciences see beyond the field as studying
determining whether the earlier acquired knowledge in life is more
memorized than the knowledge acquired in the coming age or called ‘The
Entrenchment Effect’ &lt;a class=&quot;citation&quot; href=&quot;#Mermillod2013&quot;&gt;[3]&lt;/a&gt;. Therefore, it seems a little bit different between what plasticity-stability stands for in deep learning and the cognitive science community.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;While the neural networks adapt flexibly to the new incoming knowledge,
it will serendipitously experience catastrophic forgetting. Conversely,
networks that are prone to being unable to discriminate the new incoming inputs if the networks are extremely stable or commonly known as catastrophic remembering &lt;a class=&quot;citation&quot; href=&quot;#Kaushik2021&quot;&gt;[4]&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Contemporarily, deep learning is trained on top of a weak but useful
assumption of &lt;a href=&quot;https://deepai.org/machine-learning-glossary-and-terms/independent-and-identically-distributed-random-variables&quot;&gt;i.i.d (independent and identically distributed)&lt;/a&gt; setting which means that the data points are supposed to be mutually independent  — single data is unrelated to other data point — and
having similar distribution e.g. training data is assumed to have
equivalent distribution to test data. Therefore, the common training
setting takes the batch of samples and updates the model parameters with respect to the loss value on this batch. However, the assumption is not applicable for real-time application such as sequentially data stream training settings just like continual learning and accidentally leads to catastrophic forgetting.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Shortly, catastrophic forgetting is the radical performance drops of the model $f(X;\theta)$ which parameterized by $\theta$ with input $X$ — mostly neural networks exhibit distributed representation &lt;a class=&quot;citation&quot; href=&quot;#McCloskey1989&quot;&gt;[1]&lt;/a&gt; — that map $X \rightarrow Y$ performing on previously learned tasks $t_{t}$ after learning on task $t_{n}$ where &lt;em&gt;t&lt;/em&gt; &amp;lt; &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/catastrophic_forgetting/forgetting_cl_task.jpg&quot; alt=&quot;continual learning task settings&quot; style=&quot;&quot; /&gt;
  
    &lt;figcaption&gt;
      Figure 1. Continual learning task setting is designed for the model to learning multiple tasks incrementally which each individual task encompass a set of some classes

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Consider as an illustration on figure 1 above, our neural networks train to discriminate
between two classes of cat and dog. Therefore, the network is trained on bunches of datasets containing any variants of cat and dog for some
epochs. Thereafter we want our model to recognize 2 additional classes
of tiger and elephant in task 2. Hence, we should train the model with task 2
dataset holding batches of samples of tiger and elephant. In the continual learning setting, we are not allowed to train the model on
both task datasets and getting access to the existing dataset only —
cluster of tigers and elephants images in this case. As a result, the
model will update the parameters to optimizely perform good at present
task or task 2 and forget how to predict the task 1 classes given task 1 dataset; therefore, reducing the performance on task 1 or called
catastrophic forgetting.&lt;/p&gt;

&lt;h1 id=&quot;how-do-neural-networks-forget&quot;&gt;How Do Neural Networks Forget?&lt;/h1&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Mostly the standard approach for training the neural networks model is
using standard backpropagation with gradient-based optimization in
particular stochastic gradient descent (SGD) &lt;a class=&quot;citation&quot; href=&quot;#Robbins1951&quot;&gt;[5]&lt;/a&gt; or more sophisticated one like Adam &lt;a class=&quot;citation&quot; href=&quot;#Kingma2015&quot;&gt;[6]&lt;/a&gt;. Updating parameters via SGD as below&lt;/p&gt;

\[\theta \leftarrow \theta - \eta\frac{\partial\mathcal{L}}{\partial\theta},\]

&lt;p style=&quot;text-align: justify;&quot;&gt;require $\eta$ for tuning the updating magnitude or called learning rate on the parameters gradient $\frac{\partial\mathcal{L}}{\partial\theta}$. However, these networks trained by gradient-based optimization algorithms are prone to encounter catastrophic forgetting. The common reason is coming from the primary factor of parameters drift while the neural networks train by taking steps to updating parameters aiming to minimize the loss on task $t$. Thanks to Masana et al &lt;a class=&quot;citation&quot; href=&quot;#Masana2020&quot;&gt;[7]&lt;/a&gt; briefly summarize the factors of forgetting, those are including parameters shifting, logits shifting, and Inter-domain/inter-task confusion.&lt;/p&gt;

&lt;h2 id=&quot;parameters-shifting&quot;&gt;Parameters shifting&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;While the networks are being trained on the current task, the parameters will be tuned with respect to loss value in the current training dataset task. It means that the networks are optimized to perform maximum on the current task by changing the parameters. As a result, the optimization and parameters update will not consider previous task distribution which lead to forgetting how to do preceding tasks.&lt;/p&gt;

&lt;h2 id=&quot;logits-shifting&quot;&gt;Logits shifting&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The direct ramification of parameters shifting outputs distribution deviation of the logits given the certain input e.g., image of the previous task. In the effort to alleviate this detriment, distilling the knowledge &lt;a class=&quot;citation&quot; href=&quot;#Hinton2015&quot;&gt;[8]&lt;/a&gt; of the previous model parameters respecting the old inputs squeezes the logits outputs of the current model to be equal to the previous model logits while allowing the parameters inconsistent to the old model.&lt;/p&gt;

&lt;h2 id=&quot;inter-domaininter-task-confusion&quot;&gt;Inter-domain/inter-task confusion&lt;/h2&gt;
&lt;figure class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/catastrophic_forgetting/forgetting_inter_task.svg&quot; alt=&quot;inter-task confusion&quot; style=&quot;width:75%&quot; /&gt;
  
    &lt;figcaption&gt;
      Figure 2. The networks are susceptible to misclassify task 3 (right) classes due to the model having not trained to create discriminative decision boundary for 4 classes in task 3 because of sequential learning on task 1 and task 2 separately. Source: &lt;a href=&quot;https://arxiv.org/abs/2010.15277&quot;&gt;Masana, M. et al.&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The decision boundary adjustment leading to inter-task or inter-domain
misclassification due to sequential learning setting on continual
learning.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/catastrophic_forgetting/forgetting_forgetting.svg&quot; alt=&quot;catastrophic forgetting in binary classification&quot; style=&quot;width:85%&quot; /&gt;
  
    &lt;figcaption&gt;
      Figure 3. Catastrophic forgetting in binary classification while the networks are trained on task 2 suffering from distributional shift which leads to forgetting to do discrimination on task 1. Source: &lt;a href=&quot;https://www.semanticscholar.org/paper/Attention-Based-Selective-Plasticity-Kolouri-Ketz/fd45befff6852def1ba78ef0d2cd18f5e0f62f68&quot;&gt;Kolouri, S. et al&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Take an example of a binary classification task — predicting whether
given input &lt;em&gt;X&lt;/em&gt; resulting discrete label 0 or 1 — as illustrated in
the figure 3 (missing reference) above, at the beginning the networks learn to predict the
dataset distribution on task 1 in such a way that resulting the model
$f(X;\theta_{0})$ with obtained parameters $\theta_{0}$.  Then whenever
the model acquires the new knowledge from dataset distribution on task 2 without certain continual learning technique, it will suffer from
catastrophic forgetting on distribution dataset on task 1 due to
parameters drift as consequence of distribution drift which lead to
accidentally changing the decision boundary. In contrast, the ideal case
should be like the right image in figure 3 which the model performs well by generating a decision boundary that captures discriminative features on both distributions. This setting can be conveniently achieved on &lt;a href=&quot;https://ruder.io/multi-task/&quot;&gt;multi-task learning&lt;/a&gt; settings while running the training on both dataset distributions but highly difficult for continual learning.&lt;/p&gt;

&lt;h1 id=&quot;measuring-catastrophic-forgetting&quot;&gt;Measuring Catastrophic Forgetting&lt;/h1&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;How to measure catastrophic forgetting could perhaps be separated into
two perspectives thus quantifying to what extent the networks model is
able to acquire new knowledge without forgetting and the other examine
how fast the networks models adapt to past knowledge while relearning
the past task after training on present task, both measurements called
&lt;strong&gt;retention&lt;/strong&gt; and &lt;strong&gt;relearning&lt;/strong&gt; respectively &lt;a class=&quot;citation&quot; href=&quot;#Ashley2021&quot;&gt;[9]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;retention&quot;&gt;Retention&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Retention is most commonly used as a measuring technique for continual
learning including incremental class learning or task incremental
learning in the machine learning community nowadays. Simply training the networks until mastering on task 1, then moving forward to task 2 and let the networks mastering on task 2 and followed by measuring the
accuracy metrics on task 1 and 2 independently is categorized as one of
the retention measurements &lt;a class=&quot;citation&quot; href=&quot;#Ashley2021&quot;&gt;[9]&lt;/a&gt;. Additionally, &lt;a class=&quot;citation&quot; href=&quot;#Rebuffi2017&quot;&gt;[10]&lt;/a&gt; proposed widely adopted measuring technique called &lt;strong&gt;&lt;em&gt;average incremental accuracy&lt;/em&gt;&lt;/strong&gt; as formalized by following equation&lt;/p&gt;

\[accuracy = \frac{1}{T}\sum_{t = 1}^{T}A_{t},\]

&lt;p style=&quot;text-align: justify;&quot;&gt;where &lt;em&gt;T&lt;/em&gt; is the number of tasks has been encountered so far and $A_{t}$ means accuracy on tast $t$.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;However, more complicated one has been proposed by &lt;a class=&quot;citation&quot; href=&quot;#Kemker2018&quot;&gt;[11]&lt;/a&gt; which introducing&lt;/p&gt;

\[\Omega_{\text{base}} = \frac{1}{T - 1}\sum_{i = 2}^{T}\frac{\alpha_{base,i}}{\alpha_{\text{ideal}}},\]

\[\Omega_{\text{new}} = \frac{1}{T - 1}\sum_{i = 2}^{T}\alpha_{new,i},\]

\[\Omega_{\text{all}} = \frac{1}{T - 1}\sum_{i = 2}^{T}\frac{\alpha_{all,i}}{\alpha_{\text{ideal}}},\]

&lt;p style=&quot;text-align: justify;&quot;&gt;$T$ is the total tasks/sessions have been trained so far,
$\alpha_{new,i}$ denotes accuracy on test set for session &lt;em&gt;i&lt;/em&gt; direcly after learning,$\ \alpha_{base,i}$ is the measurement of
accuracy on base class/first session after learning on sesion &lt;em&gt;i&lt;/em&gt;, while $\alpha_{all,i}$ is accuracy metric on all session given model trained on session &lt;em&gt;i&lt;/em&gt;, and $\alpha_{\text{ideal}}$ indicates the offline model accuracy on the base set, which assumes the ideal performance or sometimes many experiments in continual learning anchor multi-task learning setting as the upper-bound. While, the function of alpha ideal as divisor here for normalization for ease to compare between datasets.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;$\Omega_{\text{base}}$ indicates the model’s retention relative to the
first session given trained model in later sessions. $\Omega_{\text{new}}$ measures the accuracy on training session &lt;em&gt;i&lt;/em&gt; while the model is trained on session &lt;em&gt;i&lt;/em&gt; as well, it is used for a model’s ability to immediately recall new tasks. While, $\Omega_{\text{all}}$ denotes the measurement for how well the model retain all session after trained on session &lt;em&gt;i&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;relearning&quot;&gt;Relearning&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Frequently overlooked by existing recent experiments, relearning is
another essential measure in catastrophic forgetting which was initially proposed in physiological study by Hermann Ebbinghaus known as ‘savings’ but implemented as metrics in catastrophic forgetting by
Hetherington &lt;a class=&quot;citation&quot; href=&quot;#Hetherington1989&quot;&gt;[12]&lt;/a&gt;. ‘Saving’ metrics measure the saved knowledge and how fast the networks relearn the past knowledge. This metric is built on top of the assumption that possibly networks are not totally unlearned the past knowledge but that their connections may save encoded important information of the past.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Practically it is measured via training the network on task 1 and task 2 sequentially, then retrain the networks on task 1 dataset and compare
the time required for the network to learn task 1 on the first time
against second time. Reducing time required to relearn the task 1
indicates that the networks still saved the past information.&lt;/p&gt;

&lt;h2 id=&quot;activation-overlap&quot;&gt;Activation Overlap&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Activation overlap initially proposed by French &lt;a class=&quot;citation&quot; href=&quot;#French1993&quot;&gt;[13]&lt;/a&gt; who argue that due to distributed representation causing connectionist networks, forgetting can be measured by quantifying the overlapping in activation output. Recently, this formalized and modified by &lt;a class=&quot;citation&quot; href=&quot;#Ashley2021&quot;&gt;[9]&lt;/a&gt; by suggesting dot product of two different samples from whether intra-class or inter-class given same hidden parameters as following,&lt;/p&gt;

\[s\left( a,b \right) = \frac{1}{n}\sum_{i = 0}^{n}{g_{\text{hi}}\left( a \right)\text{.}g_{\text{hi}}\left( b \right)}\]

&lt;p style=&quot;text-align: justify;&quot;&gt;where $g_{\text{hi}}$ indicates hidden layer &lt;em&gt;i&lt;/em&gt; parameters of the
networks and $g_{\text{hi}}\left( x \right)$ indicating activation
output of input $x$ given parameters $g_{\text{hi}}$.&lt;/p&gt;

&lt;h2 id=&quot;pairwise-interference&quot;&gt;Pairwise Interference&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Initially proposed by &lt;a class=&quot;citation&quot; href=&quot;#Liu2019a&quot;&gt;[14]&lt;/a&gt; and then implemented by
&lt;a class=&quot;citation&quot; href=&quot;#Ghiassian2020&quot;&gt;[15]&lt;/a&gt; given sample &lt;em&gt;a&lt;/em&gt; and sample &lt;em&gt;b&lt;/em&gt; pairwise
interference measure how large the interference of sample &lt;em&gt;b&lt;/em&gt;  for
trained model on sample &lt;em&gt;a&lt;/em&gt; which can be defined as follow&lt;/p&gt;

\[\text{PI}\left( \theta_{t};a,b \right) = J\left( \theta_{t + 1};a \right) - \ J\left( \theta_{t};a \right).\]

&lt;p style=&quot;text-align: justify;&quot;&gt;Where, $\theta_{t + 1}$ is a model obtained after training on sample
&lt;em&gt;b&lt;/em&gt;, and $J(.)$ indicates objective function.&lt;/p&gt;

&lt;h1 id=&quot;overcoming-forgetting-in-neural-networks&quot;&gt;Overcoming Forgetting in Neural Networks&lt;/h1&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Contemporarily mitigating catastrophic forgetting highly involved in
subfield of machine learning so-called continual learning. Recent
advancement approaches in dealing with the issue encompassing
exemplar/prototypical/experience rehearsal/replay buffer, parameters
regularization, and architectural modification or otherwise named
modular approach. In spite of those, in the recent past one year some
scientists extend the study of moderating catastrophic forgetting a.k.a. continual learning to the search of connectivity with multi-task
learning &lt;a class=&quot;citation&quot; href=&quot;#Mirzadeh2020a&quot;&gt;[16]&lt;/a&gt;, loss landscape approximation &lt;a class=&quot;citation&quot; href=&quot;#Mirzadeh2020a&quot;&gt;[16]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Yin2020a&quot;&gt;[17]&lt;/a&gt;, relatedness with transfer learning &lt;a class=&quot;citation&quot; href=&quot;#Ke2020&quot;&gt;[18]&lt;/a&gt;, more challenging task settings &lt;a class=&quot;citation&quot; href=&quot;#SonglinDong2020&quot;&gt;[19]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Zhao2020&quot;&gt;[20]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Bertugli2020&quot;&gt;[21]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Caccia2020&quot;&gt;[22]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Ren2019&quot;&gt;[23]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Dhamija2021&quot;&gt;[24]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Rao2019&quot;&gt;[25]&lt;/a&gt; and even expanding beyond image classification task &lt;a class=&quot;citation&quot; href=&quot;#Joseph2020a&quot;&gt;[26]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Perez-Rua2020&quot;&gt;[27]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Zheng2021&quot;&gt;[28]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Chen2020d&quot;&gt;[29]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;rehearsalreplay&quot;&gt;Rehearsal/Replay&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Rehearsal/replay approach is dealing with catastrophic forgetting
modestly by replaying the bunch of knowledge memory of past knowledge so-called
“episodic memory”, e.g., samples of images, into the existing training
steps while learning the novel knowledge e.g., new classes. Therefore,
the catastrophic interference can be diminished as consequence of the
updating parameters in respect of considering batch of combining
existing datasets with small buffers of replayed episodic memory. Among
others this technique was mostly explored and proposed in past five
years in continual learning seeing its simplicity and effectiveness as
baseline for continual learning experiments.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/catastrophic_forgetting/forgetting_hippo.svg&quot; alt=&quot;rehearsal in the brain&quot; style=&quot;width:75%&quot; /&gt;
  
    &lt;figcaption&gt;
      Figure 4. Knowledge replays in the brain while sleeping involving the Neocortex and Hippocampus. Source: &lt;a href=&quot;https://www.semanticscholar.org/paper/Continual-Lifelong-Learning-with-Neural-Networks%3A-A-Parisi-Kemker/9ea50b3408f993853f1c5e374690e5fbe73c2a3c&quot;&gt;Parisi, G. I. et al.&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The similar mechanism also occurs in our brain when sleeping since our
brain will reactivate and rehearse the past freshly acquired knowledges
memorized periodically in hippocampus into peripheral permanent memory
in the neocortex. As suggested in theory of Complementary Learning System
(CLS) &lt;a class=&quot;citation&quot; href=&quot;#McClelland1995&quot;&gt;[30]&lt;/a&gt; shown in Figure 4 above, the Hippocampus encodes recent events or experiences via fast learning and these will be unconsciously reactivated while sleeping for gradual consolidation mechanism into neocortical memory systems.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;However, the most challenging in rehearsal approach is both how to sampling the
most significant examples and what kind of representations from the
dataset that necessarily be rehearsed into future learning phase while
minimizing catastrophic interference. Many of the latest research
concerned with this issue along with proposing novel sampling techniques
or including random sampling, uniform sampling, reservoir sampling &lt;a class=&quot;citation&quot; href=&quot;#Vitter1985&quot;&gt;[31]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Kim2020a&quot;&gt;[32]&lt;/a&gt;, distance-based sampling &lt;a class=&quot;citation&quot; href=&quot;#Pomponi2020a&quot;&gt;[33]&lt;/a&gt;, maximally interfered sampling &lt;a class=&quot;citation&quot; href=&quot;#Aljundi2019a&quot;&gt;[34]&lt;/a&gt;, among others. On the other hand, replaying expressive representations involve naïve image replay, embedding replay, anchor replay, and topological/relational replay.&lt;/p&gt;

&lt;h2 id=&quot;regularization&quot;&gt;Regularization&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Measuring the any past information, including parameters, importance
relevant to both past task loss value and accuracy metrics and
restricting the extreme updates to this information while learning is
the other strategy named regularization approach. This is conceivably
conjectured as the mechanism to control plasticity-stability dilemma of
the neural networks on the subject of continual updates. As consequent,
the restraint adopted to the information of interest, such as parameters, guarantee the minimization of interfered information
essential for the prior task.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Up till now, according to &lt;a class=&quot;citation&quot; href=&quot;#Delange2021&quot;&gt;[35]&lt;/a&gt;, some of experiments can be clustered into prior-focus/parameters-based and data-focused/logits-based regularization. Parameters-based control the model parameters distribution and plasticity-stability. While, data-focused distil the logits (model outputs before activation function) of given inputs inferenced on the present model as manoeuvre to recall past knowledge.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/catastrophic_forgetting/ewc.svg&quot; alt=&quot;optimization thru ewc&quot; style=&quot;width:75%&quot; /&gt;
  
    &lt;figcaption&gt;
      Figure 5. Training with EWC as shown on red trajectories will finding out low loss elevation in space on both task A (old) and task B (new) such that the obtained parameters are capable to perform accurately both on task A and B. Source: &lt;a href=&quot;https://www.pnas.org/content/114/13/3521&quot;&gt;Kirkpatrick et al.&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The earliest method proposed this idea was Elastic Weight Consolidation (EWC) &lt;a class=&quot;citation&quot; href=&quot;#Kirkpatrick2017a&quot;&gt;[36]&lt;/a&gt;. The basic idea is to measure weight importance for the previous task while controlling these previous weights $\theta_{A}^{*}$ and avoid significant updates to these weights via fisher information matrix $F$ as measure of the importance. While EWC minimize the loss of&lt;/p&gt;

\[\mathcal{L}\left ( \theta  \right ) = \mathcal{L}_{B}\left ( \theta \right ) + \sum_{i}^{} \frac{\lambda}{2}F_{i}\left ( \theta_{i} - \theta_{A,i}^{*} \right )^{2},\]

&lt;p style=&quot;text-align: justify;&quot;&gt;where $\mathcal{L}_{B}$ is the task B loss, $\lambda$ denotes the relation of old task to new, $i$ is each parameter index, and $\theta$ is the current parameters. As exhibited on the loss equation above, the new parameters will be enforced to close to old parameters to alleviate forgetting which the precision will be controlled by fisher information matrix $F$.&lt;/p&gt;

&lt;h2 id=&quot;architectural&quot;&gt;Architectural&lt;/h2&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;While architectural-based approach mainly concerned with constructing progressive neural networks while learning novel tasks or knowledges either by growing task-specific architecture &lt;a class=&quot;citation&quot; href=&quot;#Rusu2016a&quot;&gt;[37]&lt;/a&gt;, producing single-independent head on classifier per class/task (missing reference), or rewiring the connections in neural networks layers while incrementally learning novel tasks &lt;a class=&quot;citation&quot; href=&quot;#Wortsman2020a&quot;&gt;[38]&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/catastrophic_forgetting/progessive_networks.svg&quot; alt=&quot;progressive networks&quot; style=&quot;width:50%&quot; /&gt;
  
    &lt;figcaption&gt;
      Figure 6. Progressive networks exhibit three columns networks which each column is associated to task-specific networks e.g., networks on column 1 and 2 for performing task 1 and 2, respectively. Whilst column 3 networks solve task 3 that this networks is enabled getting access to previous leaned features. Source: &lt;a href=&quot;https://www.semanticscholar.org/paper/Progressive-Neural-Networks-Rusu-Rabinowitz/53c9443e4e667170acc60ca1b31a0ec7151fe753&quot;&gt;Rusu, A. A. et al.&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Among others is Progressive Neural Networks proposed in 2016 as depicted in the figure 6 above. The progressive networks framework proposed addressing catastrophic forgetting through evolving task-specific networks instanting on a column for working on a task being solved. Then, as the task encountered is incremental growth, the novel column networks will be introduced which the previously learned features feasibly transferred to the new networks via lateral connections. Therefore, the last task with its associated networks are allowed to exploit all the features learned so far.&lt;/p&gt;

&lt;blockquote style=&quot;text-align: justify;&quot;&gt;
  &lt;p&gt;[Notes]&lt;br /&gt;
&lt;span style=&quot;color:#8B0000&quot;&gt;If you have any disapproval, correction, and critique to this article feel free to &lt;a href=&quot;mailto:mrifkikurniawan17@gmail.com&quot;&gt;email me&lt;/a&gt;, I will happily adjusting and modifying this published contents respecting the corrections.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;McCloskey1989&quot;&gt;[1]M. McCloskey and N. J. Cohen, “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem,” &lt;i&gt;Psychology of Learning and Motivation - Advances in Research and Theory&lt;/i&gt;, vol. 24, no. C, pp. 109–165, 1989, doi: 10.1016/S0079-7421(08)60536-8. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;French1999&quot;&gt;[2]R. French, “Catastrophic forgetting in connectionist networks,” &lt;i&gt;Trends in Cognitive Sciences&lt;/i&gt;, vol. 3, no. 4, pp. 128–135, Apr. 1999, doi: 10.1016/S1364-6613(99)01294-2. [Online]. Available at: https://linkinghub.elsevier.com/retrieve/pii/S1364661399012942&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Mermillod2013&quot;&gt;[3]M. Mermillod, A. Bugaiska, and P. Bonin, “The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects,” &lt;i&gt;Frontiers in Psychology&lt;/i&gt;, vol. 4, no. August, pp. 1–3, 2013, doi: 10.3389/fpsyg.2013.00504. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kaushik2021&quot;&gt;[4]P. Kaushik, A. Gain, A. Kortylewski, and A. Yuille, “Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping,” 2021 [Online]. Available at: http://arxiv.org/abs/2102.11343&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Robbins1951&quot;&gt;[5]H. Robbins and S. Monro, “A Stochastic Approximation Method,” &lt;i&gt;The Annals of Mathematical Statistics&lt;/i&gt;, vol. 22, no. 3, pp. 400–407, 1951, doi: 10.1214/aoms/1177729586. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kingma2015&quot;&gt;[6]D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,” &lt;i&gt;3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings&lt;/i&gt;, pp. 1–15, 2015. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Masana2020&quot;&gt;[7]M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov, and J. V. D. Weijer, “Class-incremental learning : survey and performance evaluation,” &lt;i&gt;arXiv Preprint&lt;/i&gt;, pp. 1–24, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Hinton2015&quot;&gt;[8]G. Hinton, O. Vinyals, and J. Dean, “Distilling the Knowledge in a Neural Network,” pp. 1–9, 2015 [Online]. Available at: http://arxiv.org/abs/1503.02531&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ashley2021&quot;&gt;[9]D. R. Ashley, S. Ghiassian, and R. S. Sutton, “Does Standard Backpropagation Forget Less Catastrophically Than Adam?,” 2021 [Online]. Available at: http://arxiv.org/abs/2102.07686&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Rebuffi2017&quot;&gt;[10]S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “iCaRL: Incremental Classifier and Representation Learning,” in &lt;i&gt;2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 2017, vol. 2017-Janua, pp. 5533–5542, doi: 10.1109/CVPR.2017.587 [Online]. Available at: http://arxiv.org/abs/1611.07725 http://ieeexplore.ieee.org/document/8100070/&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kemker2018&quot;&gt;[11]R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan, “Measuring catastrophic forgetting in neural networks,” &lt;i&gt;32nd AAAI Conference on Artificial Intelligence, AAAI 2018&lt;/i&gt;, pp. 3390–3398, 2018. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Hetherington1989&quot;&gt;[12]P. A. Hetherington and M. S. Seidenberg, “Is there ‘catastrophic interference’ in connectionist networks,” in &lt;i&gt;Proceedings of the 11th annual conference of the cognitive science society&lt;/i&gt;, 1989, vol. 26, p. 33 [Online]. Available at: http://scholar.google.com/scholar?q=related:6OvJaVLsTzwJ:scholar.google.com/&amp;amp;hl=en&amp;amp;as_sdt=0,10#3&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;French1993&quot;&gt;[13]R. French and M. French, “Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionlst Networks,” &lt;i&gt;Proceedings of the AAAI Spring Symposium&lt;/i&gt;, no. JANUARY 1992, pp. 70–77, 1993. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Liu2019a&quot;&gt;[14]V. Liu, “Sparse Representation Neural Networks for Online Reinforcement Learning,” 2019. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ghiassian2020&quot;&gt;[15]S. Ghiassian, B. Rafiee, Y. L. Lo, and A. White, “Improving performance in reinforcement learning by breaking generalization in neural networks,” &lt;i&gt;Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS&lt;/i&gt;, vol. 2020-May, no. 1, pp. 438–446, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Mirzadeh2020a&quot;&gt;[16]S. I. Mirzadeh, M. Farajtabar, D. Gorur, R. Pascanu, and H. Ghasemzadeh, “Linear mode connectivity in multitask and continual learning,” in &lt;i&gt;Iclr 2021&lt;/i&gt;, 2020 [Online]. Available at: https://arxiv.org/abs/2010.04495 https://github.com/imirzadeh/MC-SGD&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yin2020a&quot;&gt;[17]D. Yin, M. Farajtabar, A. Li, N. Levine, and A. Mott, “Optimization and Generalization of Regularization-Based Continual Learning: a Loss Approximation Viewpoint,” pp. 1–17, Jun. 2020 [Online]. Available at: http://arxiv.org/abs/2006.10974&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ke2020&quot;&gt;[18]Z. Ke, B. Liu, and X. Huang, “Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks,” &lt;i&gt;NeurIPS&lt;/i&gt;, no. NeurIPS, pp. 1–12, 2020 [Online]. Available at: https://github.com/ZixuanKe/CAT&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;SonglinDong2020&quot;&gt;[19]Songlin Dong, X. Hong, X. Tao, X. Chang, X. Wei, and Y. Gong, “Few-Shot Class-Incremental Learning via Relation Knowledge Distillation,” in &lt;i&gt;35th AAAI Conference on Artificial Intelligence, AAAI 2021&lt;/i&gt;, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Zhao2020&quot;&gt;[20]H. Zhao, Y. Fu, X. Li, S. Li, B. Omar, and X. Li, “Few-shot class-incremental learning via feature space composition,” &lt;i&gt;arXiv&lt;/i&gt;, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bertugli2020&quot;&gt;[21]A. Bertugli, S. Vincenzi, S. Calderara, and A. Passerini, “Few-shot unsupervised continual learning through meta-examples,” &lt;i&gt;arXiv&lt;/i&gt;, no. NeurIPS, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Caccia2020&quot;&gt;[22]M. Caccia &lt;i&gt;et al.&lt;/i&gt;, “Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning,” 2020 [Online]. Available at: http://arxiv.org/abs/2003.05856 https://github.com/ElementAI/osaka&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ren2019&quot;&gt;[23]M. Ren, R. Liao, E. Fetaya, and R. S. Zemel, “Incremental few-shot learning with attention attractor networks,” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, vol. 32, no. NeurIPS, pp. 1–15, 2019 [Online]. Available at: https://github.com/renmengye/inc-few-shot-attractor-public&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Dhamija2021&quot;&gt;[24]A. R. Dhamija, T. Ahmad, J. Schwan, M. Jafarzadeh, C. Li, and T. E. Boult, “Self-Supervised Features Improve Open-World Learning,” 2021 [Online]. Available at: http://arxiv.org/abs/2102.07848&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Rao2019&quot;&gt;[25]D. Rao, F. Visin, A. A. Rusu, Y. W. Teh, R. Pascanu, and R. Hadsell, “Continual unsupervised representation learning,” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, vol. 32, 2019. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Joseph2020a&quot;&gt;[26]K. J. Joseph, J. Rajasegaran, S. Khan, F. S. Khan, V. N. Balasubramanian, and L. Shao, “Incremental object detection via meta-learning,” &lt;i&gt;arXiv&lt;/i&gt;, vol. 14, no. 8, pp. 1–8, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Perez-Rua2020&quot;&gt;[27]J. M. Perez-Rua, X. Zhu, T. M. Hospedales, and T. Xiang, “Incremental Few-Shot Object Detection,” &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, no. 1, pp. 13843–13852, 2020, doi: 10.1109/CVPR42600.2020.01386. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Zheng2021&quot;&gt;[28]E. Zheng, Q. Yu, R. Li, P. Shi, and A. Haake, “A Continual Learning Framework for Uncertainty-Aware Interactive Image Segmentation,” in &lt;i&gt;35th AAAI Conference on Artificial Intelligence, AAAI 2021&lt;/i&gt;, 2021. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Chen2020d&quot;&gt;[29]W. Chen, Y. Liu, W. Wang, T. Tuytelaars, E. M. Bakker, and M. Lew, “On the exploration of incremental learning for fine-grained image retrieval,” &lt;i&gt;arXiv&lt;/i&gt;, 2020. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;McClelland1995&quot;&gt;[30]J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly, “Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights From the Successes and Failures of Connectionist Models of Learning and Memory,” &lt;i&gt;Psychological Review&lt;/i&gt;, vol. 102, no. 3, pp. 419–457, 1995. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Vitter1985&quot;&gt;[31]J. S. Vitter, “Random sampling with a reservoir,” &lt;i&gt;ACM Transactions on Mathematical Software&lt;/i&gt;, vol. 11, no. 1, pp. 37–57, Mar. 1985, doi: 10.1145/3147.3165. [Online]. Available at: https://dl.acm.org/doi/10.1145/3147.3165&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kim2020a&quot;&gt;[32]C. D. Kim, J. Jeong, and G. Kim, “Imbalanced Continual Learning with Partitioning Reservoir Sampling,” in &lt;i&gt;ECCV&lt;/i&gt;, 2020, vol. 12358 LNCS, pp. 411–428, doi: 10.1007/978-3-030-58601-0_25. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pomponi2020a&quot;&gt;[33]J. Pomponi, S. Scardapane, V. Lomonaco, and A. Uncini, “Efficient continual learning in neural networks with embedding regularization,” &lt;i&gt;Neurocomputing&lt;/i&gt;, vol. 397, pp. 139–148, 2020, doi: 10.1016/j.neucom.2020.01.093. [Online]. Available at: https://doi.org/10.1016/j.neucom.2020.01.093&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Aljundi2019a&quot;&gt;[34]R. Aljundi &lt;i&gt;et al.&lt;/i&gt;, “Online Continual Learning with Maximally Interfered Retrieval,” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, vol. 32, no. NeurIPS 2019, Aug. 2019 [Online]. Available at: http://arxiv.org/abs/1908.04742&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Delange2021&quot;&gt;[35]M. Delange &lt;i&gt;et al.&lt;/i&gt;, “A continual learning survey: Defying forgetting in classification tasks,” &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, pp. 1–1, 2021, doi: 10.1109/TPAMI.2021.3057446. [Online]. Available at: http://arxiv.org/abs/1909.08383 https://github.com/Mattdl/CLsurvey https://ieeexplore.ieee.org/document/9349197/&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kirkpatrick2017a&quot;&gt;[36]J. Kirkpatrick &lt;i&gt;et al.&lt;/i&gt;, “Overcoming catastrophic forgetting in neural networks,” &lt;i&gt;Proceedings of the National Academy of Sciences of the United States of America&lt;/i&gt;, vol. 114, no. 13, pp. 3521–3526, 2017, doi: 10.1073/pnas.1611835114. [Online]. Available at: https://github.com/ariseff/overcoming-catastrophic https://github.com/stokesj/EWC&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Rusu2016a&quot;&gt;[37]A. A. Rusu &lt;i&gt;et al.&lt;/i&gt;, “Progressive Neural Networks,” &lt;i&gt;arXiv&lt;/i&gt;, Jun. 2016 [Online]. Available at: http://arxiv.org/abs/1606.04671&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wortsman2020a&quot;&gt;[38]M. Wortsman &lt;i&gt;et al.&lt;/i&gt;, “Supermasks in Superposition,” &lt;i&gt;arXiv&lt;/i&gt;, no. NeurIPS, 2020. &lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Muhammad Rifki Kurniawan</name><email>mrifkikurniawan17@gmail.com</email></author><category term="continual learning" /><category term="deep learning" /><category term="machine learning" /><category term="neural network" /><category term="catastrophic forgetting" /><summary type="html">Introduction to Catastrophic Forgetting in Neural Networks Explained</summary></entry><entry><title type="html">From Cuneiform to Binary, from Stone to Cloud Memory</title><link href="https://mrifkikurniawan.github.io/blog-posts/From_Cuneiform%20to_Binary,%20from_Stone_to_Cloud_Memory/" rel="alternate" type="text/html" title="From Cuneiform to Binary, from Stone to Cloud Memory" /><published>2020-03-25T00:00:00-07:00</published><updated>2021-05-03T04:23:05-07:00</updated><id>https://mrifkikurniawan.github.io/blog-posts/From_Cuneiform%20to_Binary,_from_Stone_to_Cloud_Memory</id><content type="html" xml:base="https://mrifkikurniawan.github.io/blog-posts/From_Cuneiform%20to_Binary,%20from_Stone_to_Cloud_Memory/">&lt;p&gt;[&lt;a href=&quot;https://medium.com/@rifkikurniawan17/dari-cuneiform-hingga-biner-dari-batu-hingga-cloud-memory-bagaimana-kita-menyimpan-data-dari-40fbdf5707a1&quot; title=&quot;Medium&quot;&gt;Medium&lt;/a&gt;]&lt;/p&gt;

&lt;blockquote style=&quot;text-align: justify;&quot;&gt;
  &lt;p&gt;Just imagine we lived in the 7000 years ago when no letters of alphabet A, B, C to Z were found, or we could not count the number of our fish caught because there were no known names such as numbers 1,2,3 or 9. Then how do we convey event A or information B and then, furthermore, store it so that it becomes our track record in the future?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;In the beginning, humans only relied on the most important organ of themselves as storage memory, the brain. Though the brain has 3 fundamental flaws according to Yuval Noah Harari. &lt;strong&gt;First, the brain has limited capacities&lt;/strong&gt;. Up to now, researchers are still unable to determine how much memory capacity our brains have. According to Paul Reber, professor of psychology at Northwestern University, which was reported by &lt;a href=&quot;https://www.scientificamerican.com/article/what-is-the-memory-capacity/&quot;&gt;Scientific American&lt;/a&gt;, the human brain consists of about one billion neurons, each of which is connected with 1000 other neurons; thus, forming about 3 trillion connections. If quantified in data size, it can be estimated that around 2.5 petabits (million gigabits), which means that capacity can be used to record millions of hours of video. However, it was all just an estimate; the researchers still could not determine the actual size because we do not know how to measure actual memory. Meanwhile, certain experiences require more detail so that they require more space in the brain so that other experiences are forgotten and free up to certain spaces in the brain.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;&lt;strong&gt;Secondly, the information stored in the brain will be destroyed when someone who owns the brain dies&lt;/strong&gt;. If Einstein lived 7000 years ago and assumed that he had found the postulate of the workings of the universe through the theory of general and special relativity. It is almost certain that the theory will not be known by humans of modern civilization. It might be possible to find it, but it will take several hundred years to uncover it with a large investment of research. We must find traces of historical information and data without writing, drawing, painting because writing as a manifestation of ideas that were not discovered yet at that time. Einstein’s ideas will die when Einstein also dies.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;&lt;strong&gt;And third, the human brain is adapted to store and process certain types of information&lt;/strong&gt;. By learning, fruit traders can differentiate between rotten and fresh fruits by just seeing or feeling a few parts. Or a snake charmer who becomes a master handler by studying snake behavior, when he can be subdued and not.&lt;/p&gt;

&lt;h2 id=&quot;the-emergence-of-clay-and-stone-letters-and-storage-technology&quot;&gt;The Emergence of Clay and Stone Letters and Storage Technology&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;The writings were first discovered by the Sumerian people who lived in the southern region of Mesopotamia around 3400–3000 BC. By scientists, Sumer is considered the first urban civilization in the world, Ancient Mesopotamia as a place where civilization emerged. This article was later called &lt;a href=&quot;https://www.ancient.eu/cuneiform/&quot;&gt;cuneiform&lt;/a&gt;. The geniuses there for the first time used this letter to count and record. In his book “Sapiens”, Yuval Noah Harari explained that Sumerian’s first manuscripts only contained documents about the economy, tax payment records, debt accumulation, and property ownership.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://miro.medium.com/max/620/1*J6q_C4-gbvXS2_krJdxU9A.jpeg&quot; alt=&quot;day image example&quot; width=&quot;600px&quot; /&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  Cuneiform, the oldest letter system in the world. Source: &lt;a href=&quot;https://www.historyextra.com/&quot;&gt;https://www.historyextra.com/.&lt;/a&gt;
&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The Sumerian writing system stores information with two marks printed on clay plates. However, with this breakthrough, the Sumerians were able to store far more data than any human brain. Nobody expected this article to make an important momentum of change in the civilization of all time. Until then developed and produced writing that was created in the Mesopotamian era like &lt;a href=&quot;https://www.britannica.com/topic/Epic-of-Gilgamesh&quot;&gt;The Epic of Gilgamesh&lt;/a&gt;, &lt;a href=&quot;https://www.ancient.eu/article/227/the-atrahasis-epic-the-great-flood--the-meaning-of/&quot;&gt;Atrahasis&lt;/a&gt;, &lt;a href=&quot;https://www.ancient.eu/article/215/inannas-descent-a-sumerian-tale-of-injustice/&quot;&gt;The Descent of Inanna&lt;/a&gt;, &lt;a href=&quot;https://www.ancient.eu/article/224/the-myth-of-etana/&quot;&gt;The Myth of Etana&lt;/a&gt;, and &lt;a href=&quot;https://www.ancient.eu/article/225/enuma-elish---the-babylonian-epic-of-creation---fu/&quot;&gt;The Enuma Elishiyang&lt;/a&gt; written in cuneiform. It takes several centuries of research to be understood by modern humans.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Until the middle of the 19th century, George Smith (1840–1876) and Henry Rawlinson (1810–1895) translated The Epic of Gilgamesh into English. Which then becomes &lt;a href=&quot;https://www.nationalgeographic.com/history/magazine/2018/01-02/history-gilgamesh-epic-discovery/&quot;&gt;momentum&lt;/a&gt; the opening of Mesopotamian historical data. This is the beginning of the writing which is then followed by variations of writing and language that we know today. Writing makes a man able to manifest information in his brain so that it becomes a solution to the deficiency of the brain and reduces brain function as a place to store information.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;In the Hindu-Buddhist kingdoms civilization in Indonesia, royal information such as the names of kings or stories of royal struggles is written. Imagine if this story is stored in the memories of the governors, maybe the story will not last long. And it might not be enjoyed by children and grandchildren for hundreds of years to come. However, the writing was manifested in stone which we call inscriptions and papers which when compiled into books make the age of information last longer, beyond human life.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;If the internet has been discovered, maybe a royal IT expert will create a royal website and write down all the information in it. Academics owned by the kingdom will write a biography of one of the kings and write historical events and the royal officials will always record the income and expenditure of the king’s finances through word excel from his computer. All information will be recorded and read by future generations. After that, all documents will be uploaded to cloud-based storage to secure the data.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;However, the human brain has not developed that far. The best technology for long-lasting data storage is still in the form of stones, tree trunks, and when it’s better to find books. In the 14th century, in the kingdom of &lt;a href=&quot;https://historia.id/kuno/articles/awal-mula-kerajaan-majapahit-D8J4o&quot;&gt;Majapahit&lt;/a&gt;, we can find traces left in the form of books like Kakawin &lt;a href=&quot;https://id.wikipedia.org/wiki/Kakawin_Nagarakretagama&quot;&gt;Nagarakrtagama&lt;/a&gt; by Empu Prapanca. The book makes it easy for the current generation to learn about life in Majapahit and uncover important events. This proves the availability of recorded information is very helpful in the world of research in revealing how people used to be active and working, the political and economic system, or the government system.&lt;/p&gt;

&lt;h2 id=&quot;when-binary-letters-meet-silicon-and-transistors&quot;&gt;When Binary Letters Meet Silicon and Transistors&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;At present, we are in the era of transistors, silicon, and binary numbers. Information and data can not only be recorded in writing but also images and videos. Someone who today records himself eating at a traditional flavored Javanese restaurant using his Apple smartphone and uploading it to an Instagram account will forever be a track record that can be opened 100 or 500 years later.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;As one Middle Eastern expert did, who can determine where hoax news is based on images in articles that are compared to the same images in certain articles using Google Image and tracks the biodata of a terrorist child and his affiliates from his photo and social media accounts using the same method for tracking the originality of an article.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The development of human civilization can not be separated from the human ability to simplify all forms of information into two numbers, 1 and 0. Two numbers that are enough to make information last forever. With the help of the formation of an internet network that can be regarded as a worldwide network of information people become goods that are no longer valuable because the amount is very abundant. Internet networks are revolutionizing the way information spreads, the presentation of information, and the age of information. With just a few fingers touch on the smartphone layer, one can make threats to a country.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The internet, which basically uses binary numbers in its information transmission, with pulses of the speed of light transmitted through optical fibers as it is now, makes information more quickly conveyed by the construction of complex networks. This makes information can be archived to other computers.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Today we know of Cloud technology, for example, owned by large companies such as Google, Amazon, or Alibaba. If they’re used to be a library that had to be destroyed to conquer and win the war. In the future cloud server might be the main target in winning the war.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Today we cannot only store information in written form. Even pictures and moving images (video) is the best way to store information. In the past, humans used language and writing to describe the atmosphere of events around them. There can be misinterpretations there, but since humans are able to record. Information storage of an event can be directly seen without the help of human reasoning.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;So what happens is eternity and ease in getting data and storing it. One day, 1,000 or 10,000 years to come to the next generation will not be difficult to explore and recognize human behavior now. All are available, all easily accessible. The abundance of information will shift the problem for the next generation from the initial allocation of energy to search to the need for more energy to assess and classify.&lt;/p&gt;</content><author><name>Muhammad Rifki Kurniawan</name><email>mrifkikurniawan17@gmail.com</email></author><category term="History" /><summary type="html">How human technology on memory evolved over periods</summary></entry><entry><title type="html">Industry 4.0-The Communicating Machines</title><link href="https://mrifkikurniawan.github.io/blog-posts/Industry_4.0-The_Communicating_Machines/" rel="alternate" type="text/html" title="Industry 4.0-The Communicating Machines" /><published>2020-03-25T00:00:00-07:00</published><updated>2021-05-03T04:23:05-07:00</updated><id>https://mrifkikurniawan.github.io/blog-posts/Industry_4.0-The_Communicating_Machines</id><content type="html" xml:base="https://mrifkikurniawan.github.io/blog-posts/Industry_4.0-The_Communicating_Machines/">&lt;p&gt;[&lt;a href=&quot;https://intip.in/LTg4&quot; title=&quot;Medium&quot;&gt;Medium&lt;/a&gt;]&lt;/p&gt;

&lt;h2 id=&quot;it-all-started-from-a-sewing-machine&quot;&gt;It All Started From a Sewing Machine&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;In 1589, Queen Elizabeth I refused to grant a patent on someone’s intelligent invention, he was William Lee, who was able to model hand movements when knitting on a machine. The Queen refused on the grounds that this machine would damage the conventional textile industry. After that, 200 years later this tool revolutionized the way the textile industry worked, the machine that had been rejected by the queen to be the most used machine in all textile industries at that time. By using massively this technology, the textile industry is able to mass-produce fabric in a faster time. This then became the most important momentum in initiating the first industrial revolution.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Late in the 18th century, to be exact, the first industrial revolution took place. This event has inherited steam technology and engine-based power that can replace the strength of human muscle. This first industrial revolution was based on Newton’s discovery when formulating the law of motion. Since then, the motion can be better understood and measured, which then allows humans to make steam engines that can mechanize the work previously done by humans &lt;a href=&quot;https://arxiv.org/abs/1703.09643&quot;&gt;(Xing and Marwala, 2017)&lt;/a&gt;. This first industrial revolution can be said to be the birth of a machine.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Great Britain became the first region to face this revolutionary change which later spread to Europe and America. This change has penetrated every aspect of life, not apart from the jobs and occupations which then changed the entire social order and psychology of society.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The lower-middle-class workers who rely more on the ability of muscles replaced by the machines. The first industrial revolution resulted in at least 1/3 of the total workers were laid off no work and another 1/3 working part-time because of the ability to work have been taken over by machines. The utilization of the steam machines increases the need for coal which then raises the problem most often discussed today, global warming.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://miro.medium.com/max/700/1*WCaxYRsolx7uTWmvaUwNSg.jpeg&quot; alt=&quot;day image example&quot; width=&quot;750px&quot; /&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  The rate of birth and death of the population during and after the industrial revolution &lt;a href=&quot;https://www.oup.com.au/__data/assets/pdf_file/0017/58031/Oxford-Big-Ideas-Geography-History-9-ch5-Industrial-revolution.pdf&quot;&gt;(Easton, 2013).&lt;/a&gt;
&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;From the demographic aspect, in 1750 the number of British people 6.5 million people increased by 400% in 1990 to 32.5 million people. In 1750, 80% of rural people lived in villages and 20% of people lived in cities, turned 180 degrees in 1850, 80% of people lived in cities and 20% of people lived in villages.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The habit of the people who initially farmed and relied on food and food supplies from agricultural products turned into an industrial labor society working in factories and offices. Up to now, we can see the impact of the industrial revolution from the phenomenon of urbanization and dominated employment patterns in the industrial sector.&lt;/p&gt;

&lt;h2 id=&quot;the-material-revolution-and-the-mobilization-revolution&quot;&gt;The Material Revolution and the Mobilization Revolution&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;100 years later, &lt;a href=&quot;https://en.wikipedia.org/wiki/Henry_Bessemer&quot;&gt;Sir Henry Bessemer&lt;/a&gt; discovered a new method of making steel from molten scrap metal. This method revolutionized steel manufacturing to be cheaper, increase the scale and speed of production, and reduce the need for workers in their manufacture &lt;a href=&quot;https://en-econ.tau.ac.il/sites/economy_en.tau.ac.il/files/media_server/Economics/PDF/Mini%20courses/castronovo.pdf&quot;&gt;(Mokyr, 1998)&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;As a result, the mass production of steel took place. Steel replaces iron which is proven to be weaker and more expensive. Steel which is proven to be cheaper and more durable enables the construction of railroad lines to be cheap, as a result, the acceleration of its construction can no longer be avoided. The existence of railroads and development that is increasingly equitable changes the pattern and intensity of transportation and supply chains which results in a decrease in commodity prices such as coal and steel.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;This transportation system also revolutionized the mobilization of all things, including in the dissemination of information and knowledge. This discovery allows more humans to migrate for certain purposes such as learning. This then also changes how science and technology spread. Because the cheaper the means of transportation, the more easily accessible transportation, the more people and goods move at an affordable price. As a result, economic growth has increased and competency and expertise have also increased.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Michael_Faraday&quot;&gt;Faraday&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/James_Clerk_Maxwell&quot;&gt;Maxwell&lt;/a&gt; accelerated this two-volume industrial revolution through their magnetic and electrical forces which created the generation of electricity and electromagnetism that are widely used in industrial systems in the future (&lt;a href=&quot;https://arxiv.org/abs/1703.09643&quot;&gt;Xing and Marwala, 2017&lt;/a&gt;). This electricity supports the industry for mass production (&lt;a href=&quot;https://www.amazon.com/Fourth-Industrial-Revolution-Professor-Dr-Ing-ebook/dp/B01JEMROIU&quot;&gt;Schwab, 2016&lt;/a&gt;).&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Electrification is also the cause of the creation of major changes in industrial production methods, which are called the assembly line and mass production. Elective also gives the industry the ability to be able to work for 24 hours without stopping. This means that work time is also increasing for employees. They can work up to 10–16 hours a day. This brings workers to bad conditions such as low wages, poor working conditions (poor lighting, and no machine safety). This results in widespread poverty and very high depression.&lt;/p&gt;

&lt;h2 id=&quot;communication-revolution&quot;&gt;Communication Revolution&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;Rapid mobilization has accelerated the spread of science and technology. The next era is the era of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transistor&quot;&gt;transistors&lt;/a&gt;. The transistor initiated the invention of computers and the internet. Transistors have led humans to the invention of computers that have become the fastest data processing machines ever. In the same era, the internet was discovered, the most massive distributor of information could be realized. The internet is changing the way humans spread information and knowledge.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Energy has become the basis of the second industrial revolution, while the third industrial revolution is the era of information and knowledge&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Letters used to be very useful in sending news or spreading information. It takes days and even months to wait for a letter to be delivered to a specific destination. At that time the inter-mail system was still a unity with the transportation system, so the letter was very dependent on the development of the transportation system. Likewise knowledge, someone needs to go overseas and travel hundreds of kilometers to meet a teacher and get knowledge from him. After that, just back to the area to disseminate the knowledge gained. However, since the discovery of the internet, everything has changed. It only takes a few minutes for information to be conveyed. Content and science books available on computer servers on the internet can be downloaded and studied with just one or two clicks. The internet makes the earth turn faster.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Acceleration of knowledge has implications for the acceleration of innovation. This era makes the machine capable of sensing and making decisions individually. Computer technology takes the machine to the next generation, namely machine automation. Computers carrying machines in the industry can work without human control in accordance with what has been programmed by humans. At that time, data and information transfers were only limited to one factory system.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Intersystem data transfer is still not possible due to limitations in data transmission technology. Next is the era in which these systems communicate with each other and each individual system is able to learn and develop like humans. What was this era like?&lt;/p&gt;

&lt;h2 id=&quot;industrial-revolution-40&quot;&gt;Industrial Revolution 4.0&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;Today, we are at the end of industry 3.0 and are on a journey of transition to a change in industry style that we could not have foreseen, the cyber-physical industry 4.0. The third industrial revolution has been able to make the glass doors of offices open automatically or to be able to close the reservoir water in our homes when fully charged. Then what time will we face after this?&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Imagine if we are leaving for work, then you call the car you have through a smartphone to take you and stop right in front of you. On the trip, you enjoy a cup of coffee that has been provided by the coffee maker every morning in your car and access the latest information through the screen projected on the windshield of your car. Meanwhile, your car is moving at a steady speed with self-driving capability without stopping due to traffic lights.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Yes, each car is able to communicate with each other so that it allows them to go without colliding. Such is the picture of industry 4.0 when all things have been made automatically, the next step is to make them able to communicate with each other like humans reprimand their friends when they meet or stay away from those who have problems with their health.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;In 2011 at an event titled “Hannover Mesese” the German government launched a completely new concept of the future of the industry. Digitalization of manufacturing was adopted as the theme which became the basis of this future industrial concept, they named it “Industrie 4.0”. The academics and business leaders who poured the concept did not know that the concept would spread quickly and become a future industrial concept of the world. 5 years later, the concept became viral and was widely discussed at conferences and seminars related to the concept of the future industry.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Industry 4.0 is a new paradigm in the revolutionary way of industry. This paradigm creates the realization of integration and interconnection between machines, products, components, individuals, and information technology that are united in industrial systems. Integration, interconnection, and flexibility are the principles that this series of four industrial revolutions will bring. The fourth industrial revolution can be said to be machines that can communicate with each other.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;This interaction allows traffic lights located in East Surabaya to communicate with traffic lights in West Surabaya to resolve traffic jams during work hours. Industry 4.0 can be said to be the next digitization of the manufacturing sector, which is controlled by 4 things: &lt;strong&gt;data empowerment, computing power, and connections&lt;/strong&gt;; &lt;strong&gt;analytical and business intelligence capabilities&lt;/strong&gt;; &lt;strong&gt;new ways of human interaction with machines such as touch layers and Augmented Reality&lt;/strong&gt;; and &lt;strong&gt;increased transfer of data instructions to the physical world such as robotics and 3D printers&lt;/strong&gt; (Lee et al., 2013).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://miro.medium.com/max/700/1*1bAmBkr3IhWa6fYzHL2q7Q.jpeg&quot; alt=&quot;day image example&quot; width=&quot;750px&quot; /&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  Industry Principles 4.0 (Cohen et al., 2017).
&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;In industry 4.0, data will be the most valuable. Data in the form of numbers and letters that can be interpreted into information. This interpretation will be very useful in making decisions resulting from the processing of AI or Machine Learning. E-Commerce currently for example has used data to predict demand and determine the price of goods in accordance with the number of requests and purchases that are continuously monitored from the virtual activities of its customers. This is evidence of the realization of flexibility in industry 4.0 and business principles that are customer-oriented by understanding the behavior and activities of customers.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;In the industrial era 4.0, technology development will refer to the fields of genetics, biotechnology, Artificial Intelligence, robotics, nanotechnology, and 3D printing (World Economic Forum, 2016). These technologies enable humans to work more efficiently and cheaper than they ever have. Artificial intelligence, for example, allows a computer to make decisions like an expert or expert system.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;If humans can realize this, then authenticity will quickly be distributed to all computers and machines. The ability of experts to predict the surge in demand for goods or the value of the most appropriate goods is easily applied and disseminated. We only need a hardware device that has sensors and is connected to the internet (IoT). Then the AI software that is installed on hardware downloaded through the internet will manage that data to be the most effective and efficient expert decision ever made.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://miro.medium.com/max/700/1*IkyTHjJcljz9gSCrrv3qvw.jpeg&quot; alt=&quot;day image example&quot; width=&quot;950px&quot; /&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  Application of industrial sectors 4.0. (World Economic Forum and International BVL, 2017).
&lt;/p&gt;

&lt;h2 id=&quot;from-centralization-towards-decentralization&quot;&gt;From Centralization Towards Decentralization&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;These years maybe the company’s production operations are still centralized. In this case, they need a fast distribution system to send their products. However, the next few years 3D printing will change it. 3D printing allows an automotive factory to produce goods in places that are affordable to customers. This will be able to minimize the distribution budget from the previous era.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Prediction using AI will enable companies to determine the best production points, complete with the number of items needed by the customer at that time. Here, 3D printers that have been placed at certain points will work according to requests sent from AI processed data. Each printer works according to the design that the company has sent to its software through a fiber optic communication network that connects the points of the production plant.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Changes in the pattern of production that are centralized in one point of production into a number of points of production which is a forecast of future production patterns. Each machine installed with AI software or machine learning will be able to make decisions individually based on centralized collective data (integration).&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;So that intelligent machines this time will be able to replace not only menial work but also the ability of logic and human thinking. The World Economic Forum with BVL International predicts the impact of the fourth industrial revolution on certain sectors such as employment, manufacturing and industry, supply chain, services and business models, and the company’s relationship with customers. Industry 4.0 will change the most essential things that humans have (&lt;a href=&quot;https://www.amazon.com/Fourth-Industrial-Revolution-Professor-Dr-Ing-ebook/dp/B01JEMROIU&quot;&gt;Schwab, 2016&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Then what about humans? Are they ready to beat the expertise of machines and robots or even get rid of the robots they make?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;p&gt;Cohen, Y. et al. (2017) ‘Assembly system configuration through Industry 4.0 principles: the expected change in the actual paradigms’, IFAC-PapersOnLine. Elsevier B.V., 50(1), pp. 14958–14963. doi: 10.1016/j.ifacol.2017.08.2550.&lt;/p&gt;

&lt;p&gt;Easton, M. (2013) The Industrial Revolution — Oxford Big Ideas Geography. Oxford.&lt;/p&gt;

&lt;p&gt;Mokyr, J. (1998) ‘The Second Industrial Revolution, 1870–1914’, (August 1998), pp. 1–19.&lt;/p&gt;

&lt;p&gt;Schwab, K. (2016) ‘The Fourth Industrial Revolution’, World Economic Forum, p. 199. doi: 10.1017/CBO9781107415324.004.&lt;/p&gt;

&lt;p&gt;World Economic Forum (2016) ‘The Future of Jobs Employment, Skills and Workforce Strategy for the Fourth Industrial Revolution’, Growth Strategies, (january), pp. 2–3. doi: 10.1177/1946756712473437.&lt;/p&gt;

&lt;p&gt;World Economic Forum and BVL Interntional (2017) ‘Impact of the Fourth Industrial Revolution on Supply Chains’, (October), p. 22.&lt;/p&gt;

&lt;p&gt;Xing, B. and Marwala, T. (2017) ‘Implications of the Fourth Industrial Age on Higher Education’, ResearchGate, (April), pp. 2–9. Available at: https://www.researchgate.net/publication/315682580%0D.&lt;/p&gt;

&lt;p&gt;http://webs.bcp.org/sites/vcleary/modernworldhistorytextbook/industrialrevolution/ireffects.html&lt;/p&gt;

&lt;p&gt;http://ushistoryscene.com/article/second-industrial-revolution/&lt;/p&gt;

&lt;p&gt;http://www.historyofinformation.com/expanded.php?id=3634&lt;/p&gt;</content><author><name>Muhammad Rifki Kurniawan</name><email>mrifkikurniawan17@gmail.com</email></author><category term="Technology" /><category term="4th Industrial Revolution" /><category term="Industrial Revolution" /><summary type="html">Industrial revolution 4.0 and Its Implications</summary></entry></feed>